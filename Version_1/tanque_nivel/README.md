# ğŸ—ï¸ SimulaciÃ³n de Control de Nivel de Tanque

Simulador fÃ­sico simple para entrenar y probar agentes de control PID usando Deep Q-Network (DQN).

## ğŸ“ Estructura

```
tanque_nivel/
â”œâ”€â”€ tanque_simulator.py    # Simulador fÃ­sico del tanque
â”œâ”€â”€ train_dqn.py          # Script de entrenamiento DQN
â””â”€â”€ README.md             # Este archivo
```

## ğŸ¯ Objetivo

Entrenar un agente DQN para controlar el nivel de lÃ­quido en un tanque cilÃ­ndrico mediante la manipulaciÃ³n del caudal de entrada.

## âš™ï¸ FÃ­sica del Proceso

### Sistema:
- **Tanque cilÃ­ndrico** con Ã¡rea constante (2 mÂ²)
- **Caudal de entrada**: Controlado por vÃ¡lvula (0-10 L/s)
- **Caudal de salida**: Por gravedad `Qout = C * âˆšh`
- **Variable controlada**: Nivel del lÃ­quido (0-5 m)

### EcuaciÃ³n diferencial:
```
A * dh/dt = Qin - Qout
```

### CaracterÃ­sticas:
- **Respuesta**: No-lineal (salida depende de âˆšh)
- **Estabilidad**: Naturalmente estable
- **Tiempo de respuesta**: ~60-120 segundos

## ğŸ§  Agente DQN

### Estado (6 dimensiones):
```python
state = [
    nivel_actual,      # PV (Process Variable)
    setpoint,          # SP (Set Point) 
    error,             # SP - PV
    error_anterior,    # Para cÃ¡lculo de derivada
    integral_error,    # Para cÃ¡lculo de integral
    derivada_error     # Derivada del error
]
```

### AcciÃ³n:
- **Espacio discreto**: 64 combinaciones de parÃ¡metros PID
- **ConversiÃ³n**: Ãndice â†’ [Kp, Ki, Kd] â†’ SeÃ±al de control
- **Control PID**: `u = Kp*e + Ki*âˆ«e + Kd*de/dt`

### Recompensa:
```python
# Recompensa principal: exponencial del error
reward = exp(-|error| * 2.0)

# Penalizaciones por niveles peligrosos
if nivel < 0.5m: penalty = -2.0 * (0.5 - nivel)
if nivel > 4.5m: penalty = -2.0 * (nivel - 4.5)
```

## ğŸš€ Uso

### Entrenamiento:
```bash
cd simulations/tanque_nivel
python train_dqn.py
```

### Prueba rÃ¡pida del simulador:
```bash
python tanque_simulator.py
```

### ParÃ¡metros principales:

**Simulador:**
- `tank_area`: Ãrea del tanque (mÂ²)
- `max_height`: Altura mÃ¡xima (m)
- `max_inflow`: Caudal mÃ¡ximo entrada (L/s)
- `noise_level`: Ruido en mediciÃ³n (%)

**Agente DQN:**
- `lr`: Learning rate (0.001)
- `gamma`: Factor descuento (0.99)
- `epsilon_start/end`: ExploraciÃ³n inicial/final
- `memory_size`: TamaÃ±o buffer replay (10000)
- `batch_size`: TamaÃ±o batch entrenamiento (32)

**Entrenamiento:**
- `num_episodes`: Episodios totales (500)
- `max_episode_steps`: Steps mÃ¡ximos por episodio (200)

## ğŸ“Š Resultados Esperados

### Durante entrenamiento:
1. **Epsilon decay**: De 1.0 â†’ 0.01 (exploraciÃ³n â†’ explotaciÃ³n)
2. **Recompensa**: Incremento gradual conforme aprende
3. **Error promedio**: DisminuciÃ³n progresiva
4. **Convergencia**: ~300-500 episodios

### Agente entrenado:
- **Error estacionario**: < 0.1m
- **Tiempo de establecimiento**: < 100 segundos
- **Sobrepico**: < 10%
- **Robustez**: Maneja perturbaciones y ruido

## ğŸ”§ PersonalizaciÃ³n

### Cambiar complejidad del proceso:
```python
# MÃ¡s ruido
TankLevelSimulator(noise_level=0.05)  # 5% ruido

# Tanque mÃ¡s rÃ¡pido
TankLevelSimulator(tank_area=1.0)  # Ãrea menor

# Mayor no-linealidad
TankLevelSimulator(outflow_coeff=3.0)  # Salida mÃ¡s agresiva
```

### Ajustar DQN:
```python
# ExploraciÃ³n mÃ¡s conservadora
DQN_Agent(epsilon_decay=0.999)

# Red mÃ¡s grande
DQN_Network(hidden_size=256)

# Memoria mÃ¡s grande
DQN_Agent(memory_size=50000)
```

## ğŸ“ˆ MÃ©tricas de EvaluaciÃ³n

El entrenador guarda automÃ¡ticamente:
- **GrÃ¡ficos de progreso**: Cada 25 episodios
- **Modelos**: Cada 50 episodios
- **MÃ©tricas**: Recompensa, error, epsilon, duraciÃ³n

### Archivos generados:
```
models_tank_dqn/
â”œâ”€â”€ dqn_tank_ep50.pth
â”œâ”€â”€ dqn_tank_ep100.pth
â”œâ”€â”€ ...
â”œâ”€â”€ dqn_tank_final.pth
â”œâ”€â”€ progress_ep25.png
â”œâ”€â”€ training_final.png
â””â”€â”€ test_episode_3.png
```

## ğŸ¯ PrÃ³ximos Pasos

1. **Validar funcionamiento bÃ¡sico**: 500 episodios
2. **Optimizar hiperparÃ¡metros**: Grid search
3. **Agregar complejidad**: Perturbaciones, delays
4. **Comparar algoritmos**: PPO, SAC, etc.
5. **Implementar en proceso real**: Hardware-in-the-loop

## ğŸ” Debugging

### Problemas comunes:

**No converge:**
- Reducir learning rate
- Aumentar epsilon_decay (explorar mÃ¡s tiempo)
- Verificar recompensa (debe ser positiva en promedio)

**Oscilaciones:**
- Ajustar parÃ¡metros PID del espacio de acciones
- Reducir ruido en simulador
- Aumentar target_update_freq

**Memoria insuficiente:**
- Reducir batch_size
- Reducir memory_size
- Usar CPU en lugar de GPU para pruebas

**Simulador inestable:**
- Verificar lÃ­mites fÃ­sicos
- Ajustar paso de simulaciÃ³n (dt)
- Revisar ecuaciÃ³n diferencial