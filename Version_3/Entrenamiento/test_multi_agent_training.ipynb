{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent PID Control - Entrenamiento y Visualizaci√≥n\n",
    "\n",
    "Este notebook prueba la arquitectura multi-agente para control PID adaptativo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports y Configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports del proyecto\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List\n",
    "\n",
    "# Arquitectura multi-agente\n",
    "from multi_agent_env_modular import MultiAgentPIDEnv\n",
    "\n",
    "# Ambientes\n",
    "from Environment.simulation_env import SimulationPIDEnv\n",
    "from Simuladores.tanque_simple import TankSimulator\n",
    "\n",
    "print(\"‚úÖ Imports completados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuraci√≥n del Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n para MODO DIRECTO (m√°s simple para empezar)\n",
    "config = {\n",
    "    # Modo de operaci√≥n\n",
    "    'mode': 'direct',\n",
    "    \n",
    "    # Variables\n",
    "    'n_manipulable_vars': 2,  # 2 variables a controlar\n",
    "    'n_variables': 2,\n",
    "    \n",
    "    # Entrenamiento\n",
    "    'n_episodes': 50,  # Episodios por agente (empezar con pocos)\n",
    "    'j_max_retries': 3,  # Reintentos de validaci√≥n\n",
    "    \n",
    "    # Par√°metros del ambiente\n",
    "    'upper_range': [100.0, 90.0],\n",
    "    'lower_range': [0.0, 0.0],\n",
    "    'setpoint': [50.0, 45.0],\n",
    "    'dead_band': [2.0, 2.0],\n",
    "    'dt': 1.0,\n",
    "    'max_episode_steps': 100,\n",
    "    \n",
    "    # Par√°metros de agentes DQN\n",
    "    'agent_lr': 0.001,\n",
    "    'agent_gamma': 0.99,\n",
    "    'epsilon_start': 1.0,\n",
    "    'epsilon_min': 0.01,\n",
    "    'epsilon_decay': 0.995,\n",
    "    'initial_pid': (1.0, 0.1, 0.05),\n",
    "    'device': 'cpu'  # Cambiar a 'cuda' si hay GPU\n",
    "}\n",
    "\n",
    "print(\"Configuraci√≥n:\")\n",
    "print(f\"  Modo: {config['mode']}\")\n",
    "print(f\"  Variables: {config['n_manipulable_vars']}\")\n",
    "print(f\"  Episodios: {config['n_episodes']}\")\n",
    "print(f\"  Reintentos: {config['j_max_retries']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crear Ambiente Multi-Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear ambiente multi-agente\n",
    "multi_env = MultiAgentPIDEnv(config)\n",
    "\n",
    "# Conectar simulador externo (opcional)\n",
    "# tank = TankSimulator(area=1.0, cv=0.1, max_height=10.0, max_flow_in=0.5, dt=1.0)\n",
    "# multi_env.base_env.connect_external_process(tank)\n",
    "\n",
    "print(\"\\n‚úÖ Ambiente multi-agente creado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar agentes\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INICIANDO ENTRENAMIENTO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_pids, best_setpoints = multi_env.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENTRENAMIENTO FINALIZADO\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nPIDs optimizados:\")\n",
    "for i, pid in enumerate(best_pids):\n",
    "    print(f\"  Variable {i}: Kp={pid[0]:.4f}, Ki={pid[1]:.4f}, Kd={pid[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluaci√≥n y Recolecci√≥n de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pid(env, pid, var_idx, setpoint, n_steps=100):\n",
    "    \"\"\"\n",
    "    Evaluar un PID en el ambiente y recolectar datos.\n",
    "    \n",
    "    Returns:\n",
    "        dict con 'pv', 'sp', 'error', 'actions'\n",
    "    \"\"\"\n",
    "    # Configurar ambiente\n",
    "    env.set_setpoint(setpoint, var_idx=var_idx)\n",
    "    env.pid_action_space.set_pid(pid[0], pid[1], pid[2])\n",
    "    \n",
    "    # Reset\n",
    "    obs, info = env.reset()\n",
    "    \n",
    "    # Recolectar datos\n",
    "    trajectory = {\n",
    "        'pv': [],\n",
    "        'sp': [],\n",
    "        'error': [],\n",
    "        'actions': []\n",
    "    }\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        # Acci√≥n 6 = mantener PID\n",
    "        obs, reward, terminated, truncated, info = env.step(6)\n",
    "        \n",
    "        # Guardar datos\n",
    "        trajectory['pv'].append(obs[0])\n",
    "        trajectory['sp'].append(obs[1])\n",
    "        trajectory['error'].append(obs[2])\n",
    "        \n",
    "        if 'control_outputs' in info:\n",
    "            trajectory['actions'].append(info['control_outputs'][var_idx])\n",
    "        else:\n",
    "            trajectory['actions'].append(0.0)\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "# Evaluar cada agente\n",
    "print(\"\\nEvaluando PIDs optimizados...\")\n",
    "trajectories = []\n",
    "\n",
    "for i in range(config['n_manipulable_vars']):\n",
    "    print(f\"  Evaluando Variable {i}...\")\n",
    "    traj = evaluate_pid(\n",
    "        env=multi_env.base_env,\n",
    "        pid=best_pids[i],\n",
    "        var_idx=i,\n",
    "        setpoint=config['setpoint'][i],\n",
    "        n_steps=100\n",
    "    )\n",
    "    trajectories.append(traj)\n",
    "\n",
    "print(\"‚úÖ Evaluaci√≥n completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizaci√≥n - PV vs SP por Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear gr√°fico de PV vs SP para cada agente\n",
    "n_agents = config['n_manipulable_vars']\n",
    "\n",
    "fig, axes = plt.subplots(n_agents, 1, figsize=(12, 5*n_agents))\n",
    "\n",
    "# Si solo hay un agente, convertir a lista\n",
    "if n_agents == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i in range(n_agents):\n",
    "    ax = axes[i]\n",
    "    traj = trajectories[i]\n",
    "    dead_band = config['dead_band'][i]\n",
    "    \n",
    "    # PV y Setpoint\n",
    "    ax.plot(traj['pv'], 'b-', label='PV (Nivel)', linewidth=2)\n",
    "    ax.plot(traj['sp'], 'r--', label='Setpoint', linewidth=2)\n",
    "    \n",
    "    # Banda muerta\n",
    "    ax.fill_between(\n",
    "        range(len(traj['pv'])),\n",
    "        [s - dead_band for s in traj['sp']],\n",
    "        [s + dead_band for s in traj['sp']],\n",
    "        alpha=0.2,\n",
    "        color='green',\n",
    "        label='Dead Band'\n",
    "    )\n",
    "    \n",
    "    # Configuraci√≥n\n",
    "    ax.set_xlabel('Step', fontsize=12)\n",
    "    ax.set_ylabel('Valor', fontsize=12)\n",
    "    ax.set_title(\n",
    "        f'Agente Controlador {i} - PV vs Setpoint\\n'\n",
    "        f'PID: Kp={best_pids[i][0]:.3f}, Ki={best_pids[i][1]:.3f}, Kd={best_pids[i][2]:.3f}',\n",
    "        fontsize=14,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "    ax.legend(fontsize=11, loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pv_vs_sp_agents.png', dpi=150, bbox_inches='tight')\n",
    "print(\"üìä Gr√°fico guardado: pv_vs_sp_agents.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizaci√≥n - Error por Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear gr√°fico de error para cada agente\n",
    "fig, axes = plt.subplots(n_agents, 1, figsize=(12, 4*n_agents))\n",
    "\n",
    "if n_agents == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i in range(n_agents):\n",
    "    ax = axes[i]\n",
    "    traj = trajectories[i]\n",
    "    dead_band = config['dead_band'][i]\n",
    "    \n",
    "    # Error\n",
    "    ax.plot(traj['error'], 'r-', linewidth=2)\n",
    "    ax.axhline(y=0, color='k', linestyle='--', alpha=0.5, linewidth=2)\n",
    "    ax.axhline(y=dead_band, color='g', linestyle=':', alpha=0.5, label='Dead Band')\n",
    "    ax.axhline(y=-dead_band, color='g', linestyle=':', alpha=0.5)\n",
    "    \n",
    "    # Configuraci√≥n\n",
    "    ax.set_xlabel('Step', fontsize=12)\n",
    "    ax.set_ylabel('Error', fontsize=12)\n",
    "    ax.set_title(f'Agente Controlador {i} - Error de Control', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('error_agents.png', dpi=150, bbox_inches='tight')\n",
    "print(\"üìä Gr√°fico guardado: error_agents.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizaci√≥n - Acciones de Control por Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear gr√°fico de acciones para cada agente\n",
    "fig, axes = plt.subplots(n_agents, 1, figsize=(12, 4*n_agents))\n",
    "\n",
    "if n_agents == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i in range(n_agents):\n",
    "    ax = axes[i]\n",
    "    traj = trajectories[i]\n",
    "    \n",
    "    # Acci√≥n de control\n",
    "    ax.plot(traj['actions'], 'purple', linewidth=2)\n",
    "    ax.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Configuraci√≥n\n",
    "    ax.set_xlabel('Step', fontsize=12)\n",
    "    ax.set_ylabel('Acci√≥n de Control', fontsize=12)\n",
    "    ax.set_title(\n",
    "        f'Agente Controlador {i} - Salida del PID',\n",
    "        fontsize=14,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('actions_agents.png', dpi=150, bbox_inches='tight')\n",
    "print(\"üìä Gr√°fico guardado: actions_agents.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Estad√≠sticas Finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener estad√≠sticas\n",
    "stats = multi_env.get_statistics()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ESTAD√çSTICAS FINALES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nPIDTrainer:\")\n",
    "for key, value in stats['pid_trainer'].items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nStabilityCriteria:\")\n",
    "for key, value in stats['stability_criteria'].items():\n",
    "    if isinstance(value, float):\n",
    "        if 'rate' in key:\n",
    "            print(f\"  {key}: {value:.2%}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Resumen de Performance por Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN DE PERFORMANCE POR AGENTE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(n_agents):\n",
    "    traj = trajectories[i]\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    final_error = abs(traj['error'][-1])\n",
    "    max_error = max(abs(e) for e in traj['error'])\n",
    "    avg_error = np.mean([abs(e) for e in traj['error']])\n",
    "    \n",
    "    # Tiempo de establecimiento (aprox)\n",
    "    dead_band = config['dead_band'][i]\n",
    "    settling_time = None\n",
    "    for t, e in enumerate(traj['error']):\n",
    "        if abs(e) <= dead_band:\n",
    "            settling_time = t\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nAgente Controlador {i}:\")\n",
    "    print(f\"  PID: Kp={best_pids[i][0]:.4f}, Ki={best_pids[i][1]:.4f}, Kd={best_pids[i][2]:.4f}\")\n",
    "    print(f\"  Error final: {final_error:.4f}\")\n",
    "    print(f\"  Error m√°ximo: {max_error:.4f}\")\n",
    "    print(f\"  Error promedio: {avg_error:.4f}\")\n",
    "    if settling_time is not None:\n",
    "        print(f\"  Tiempo de establecimiento: {settling_time} steps\")\n",
    "    else:\n",
    "        print(f\"  Tiempo de establecimiento: No alcanzado\")\n",
    "    \n",
    "    # √âxito\n",
    "    success = final_error <= dead_band\n",
    "    print(f\"  Estado: {'‚úÖ √âXITO' if success else '‚ö†Ô∏è  FUERA DE BANDA'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. (OPCIONAL) Modo Indirecto con Orquestador\n",
    "\n",
    "Descomentar para probar modo indirecto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Configuraci√≥n para MODO INDIRECTO\n",
    "# config_indirect = {\n",
    "#     'mode': 'indirect',\n",
    "#     \n",
    "#     # Variables manipulables\n",
    "#     'n_manipulable_vars': 2,\n",
    "#     'n_variables': 2,\n",
    "#     \n",
    "#     # Variables objetivo\n",
    "#     'n_target_vars': 1,\n",
    "#     'target_ranges': [(0.0, 10.0)],\n",
    "#     'target_setpoints': [5.0],\n",
    "#     \n",
    "#     # Entrenamiento\n",
    "#     'n_episodes': 30,\n",
    "#     'j_max_retries': 3,\n",
    "#     'r_orchestrator_iterations': 10,\n",
    "#     \n",
    "#     # Rangos operativos\n",
    "#     'sp_ranges': [(40.0, 60.0), (35.0, 55.0)],\n",
    "#     \n",
    "#     # Par√°metros ambiente\n",
    "#     'upper_range': [100.0, 90.0],\n",
    "#     'lower_range': [0.0, 0.0],\n",
    "#     'setpoint': [50.0, 45.0],\n",
    "#     'dead_band': [2.0, 2.0],\n",
    "#     'dt': 1.0,\n",
    "#     'max_episode_steps': 100,\n",
    "#     \n",
    "#     # Par√°metros agentes\n",
    "#     'agent_lr': 0.001,\n",
    "#     'agent_gamma': 0.99,\n",
    "#     'orch_lr_actor': 0.0001,\n",
    "#     'orch_lr_critic': 0.001,\n",
    "#     'orch_gamma': 0.99,\n",
    "#     'device': 'cpu'\n",
    "# }\n",
    "\n",
    "# # Crear y entrenar\n",
    "# multi_env_indirect = MultiAgentPIDEnv(config_indirect)\n",
    "# best_pids_ind, best_sps = multi_env_indirect.train()\n",
    "\n",
    "# print(f\"\\nMejores PIDs: {best_pids_ind}\")\n",
    "# print(f\"Mejores Setpoints: {best_sps}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
