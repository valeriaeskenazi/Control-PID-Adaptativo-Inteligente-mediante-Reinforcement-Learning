{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè≠ Entrenamiento Completo - CSTR (CTRL + ORCH)\n",
    "\n",
    "**Pipeline completo**:\n",
    "1. **Fase 1**: Entrenar agente CTRL para controlar Tc y F del CSTR\n",
    "2. **Fase 2**: Entrenar agente ORCH para optimizar Cb usando el CTRL entrenado\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Imports y Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Agregar path del proyecto si es necesario\n",
    "sys.path.append('../')\n",
    "\n",
    "from Environment.Simulation_Env.Reactor_CSTR import CSTRSimulator\n",
    "from Environment.PIDControlEnv_simple import PIDControlEnv_Simple\n",
    "from Environment.PIDControlEnv_complex import PIDControlEnv_Complex\n",
    "from Agente.DQN.train_DQN import DQNTrainer\n",
    "from Aux.Plots import SimplePlotter, print_summary\n",
    "\n",
    "print(\"‚úÖ Imports completados\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device disponible: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üéØ FASE 1: Entrenar Agente CTRL\n",
    "\n",
    "**Objetivo**: Entrenar un agente que ajuste par√°metros PID para controlar Tc y F.\n",
    "\n",
    "**Variables**:\n",
    "- **Manipulables**: Tc (290-450 K), F (99-105 m¬≥/s)\n",
    "- **Setpoints**: Valores aleatorios dentro de los rangos\n",
    "- **Agente**: Aprende a ajustar (Kp, Ki, Kd) para cada variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuraci√≥n CTRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_ctrl = {\n",
    "    # ============ AMBIENTE ============\n",
    "    'env_config': {\n",
    "        'architecture': 'simple',\n",
    "        'env_type': 'simulation',\n",
    "        \n",
    "        # 2 variables manipulables (Tc, F)\n",
    "        'n_manipulable_vars': 2,\n",
    "        'manipulable_ranges': [\n",
    "            (290, 450),  # Tc [K]\n",
    "            (99, 105)    # F [m¬≥/s]\n",
    "        ],\n",
    "        \n",
    "        # Los setpoints se generan aleatoriamente\n",
    "        'manipulable_setpoints': None,  # Random\n",
    "        \n",
    "        'dt_usuario': 1.0,\n",
    "        'max_steps': 100,\n",
    "        \n",
    "        'agent_controller_config': {\n",
    "            'agent_type': 'discrete'\n",
    "        },\n",
    "        \n",
    "        # Simulador CSTR\n",
    "        'env_type_config': {\n",
    "            'dt': 1.0,\n",
    "            'control_limits': ((290, 450), (99, 105))\n",
    "        },\n",
    "        \n",
    "        # Pesos de recompensa\n",
    "        'reward_weights': {\n",
    "            'error': 1.0,\n",
    "            'tiempo': 0.01,\n",
    "            'overshoot': 0.5,\n",
    "            'energy': 0.1\n",
    "        },\n",
    "        'reward_dead_band': 0.02,\n",
    "        \n",
    "        # L√≠mites PID\n",
    "        'pid_limits': [\n",
    "            (0.01, 50.0),\n",
    "            (0.0, 5.0),\n",
    "            (0.0, 5.0)\n",
    "        ],\n",
    "        'delta_percent_ctrl': 0.2,\n",
    "    },\n",
    "    \n",
    "    # ============ AGENTE CTRL ============\n",
    "    'agent_ctrl_config': {\n",
    "        'state_dim': 10,   # 5 obs √ó 2 variables\n",
    "        'action_dim': 7,\n",
    "        'n_vars': 2,\n",
    "        'hidden_dims': (128, 64),\n",
    "        'lr': 0.001,\n",
    "        'gamma': 0.99,\n",
    "        'epsilon_start': 1.0,\n",
    "        'epsilon_min': 0.01,\n",
    "        'epsilon_decay': 0.995,\n",
    "        'batch_size': 32,\n",
    "        'target_update_freq': 100,\n",
    "        'buffer_type': 'simple',\n",
    "        'buffer_size': 10000,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'seed': 42\n",
    "    },\n",
    "    \n",
    "    # ============ ENTRENAMIENTO ============\n",
    "    'n_episodes': 300,\n",
    "    'max_steps_per_episode': 100,\n",
    "    'eval_frequency': 50,\n",
    "    'save_frequency': 9999,\n",
    "    'log_frequency': 10,\n",
    "    'checkpoint_dir': 'checkpoints/cstr_ctrl',\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n CTRL creada\")\n",
    "print(f\"  Episodios: {config_ctrl['n_episodes']}\")\n",
    "print(f\"  Device: {config_ctrl['agent_ctrl_config']['device']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Test Ambiente Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ambiente\n",
    "test_env = PIDControlEnv_Simple(config_ctrl['env_config'])\n",
    "sim_cfg = config_ctrl['env_config']['env_type_config']\n",
    "simulator = CSTRSimulator(**sim_cfg)\n",
    "test_env.proceso.connect_external_process(simulator)\n",
    "print('‚úÖ CSTR conectado al ambiente simple')\n",
    "\n",
    "# Reset y step de prueba\n",
    "obs, info = test_env.reset()\n",
    "print(f\"\\nüìä Observaci√≥n shape: {obs.shape}\")\n",
    "print(f\"  PVs iniciales: {test_env.manipulable_pvs}\")\n",
    "print(f\"  Setpoints: {test_env.manipulable_setpoints}\")\n",
    "\n",
    "# Step aleatorio\n",
    "action = test_env.action_space.sample()\n",
    "obs, reward, done, truncated, info = test_env.step(action)\n",
    "print(f\"\\n‚úÖ Step de prueba exitoso\")\n",
    "print(f\"  Reward: {reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Entrenar CTRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear trainer CTRL\n",
    "trainer_ctrl = DQNTrainer(config_ctrl)\n",
    "\n",
    "# Conectar CSTR\n",
    "sim_cfg = config_ctrl['env_config']['env_type_config']\n",
    "simulator = CSTRSimulator(**sim_cfg)\n",
    "trainer_ctrl.env.proceso.connect_external_process(simulator)\n",
    "print('‚úÖ CSTR conectado al trainer CTRL')\n",
    "\n",
    "print(\"\\nüöÄ Entrenando agente CTRL...\\n\")\n",
    "trainer_ctrl.train()\n",
    "print(\"\\n‚úÖ Entrenamiento CTRL completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Visualizar Resultados CTRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = SimplePlotter()\n",
    "\n",
    "# Resumen\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä RESULTADOS CTRL\")\n",
    "print_summary(\n",
    "    episode_rewards=trainer_ctrl.episode_rewards,\n",
    "    episode_energies=trainer_ctrl.episode_energies,\n",
    "    episode_max_overshoots=trainer_ctrl.episode_max_overshoots,\n",
    "    best_episode_idx=np.argmax(trainer_ctrl.episode_rewards)\n",
    ")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Gr√°ficos\n",
    "plotter.plot_training_overview(\n",
    "    episode_rewards=trainer_ctrl.episode_rewards,\n",
    "    episode_energies=trainer_ctrl.episode_energies,\n",
    "    episode_max_overshoots=trainer_ctrl.episode_max_overshoots,\n",
    "    epsilons=trainer_ctrl.epsilons,\n",
    "    window=20\n",
    ")\n",
    "\n",
    "# Evoluci√≥n de PIDs\n",
    "plotter.plot_pid_evolution(\n",
    "    kp_history=trainer_ctrl.kp_history,\n",
    "    ki_history=trainer_ctrl.ki_history,\n",
    "    kd_history=trainer_ctrl.kd_history,\n",
    "    var_name=\"CSTR (Tc, F)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Guardar Checkpoint CTRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelo CTRL\n",
    "checkpoint_dir = Path(config_ctrl['checkpoint_dir'])\n",
    "checkpoint_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "ctrl_checkpoint_path = checkpoint_dir / 'agent_ctrl_best.pt'\n",
    "trainer_ctrl.agent_ctrl.save(str(ctrl_checkpoint_path))\n",
    "\n",
    "print(f\"‚úÖ Agente CTRL guardado en: {ctrl_checkpoint_path}\")\n",
    "print(f\"\\nüíæ Este checkpoint se usar√° para entrenar ORCH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üéØ FASE 2: Entrenar Agente ORCH\n",
    "\n",
    "**Objetivo**: Entrenar un agente orquestador que optimice la concentraci√≥n Cb del reactor.\n",
    "\n",
    "**Arquitectura**:\n",
    "- **ORCH**: Observa Cb, decide setpoints para Tc y F\n",
    "- **CTRL** (pre-entrenado): Ajusta PIDs para alcanzar esos setpoints\n",
    "- **CSTR**: Procesa Tc y F ‚Üí genera Cb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Configuraci√≥n ORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_orch = {\n",
    "    # ============ AMBIENTE ============\n",
    "    'env_config': {\n",
    "        'architecture': 'jerarquica',\n",
    "        'env_type': 'simulation',\n",
    "        \n",
    "        # Variables manipulables (controladas por CTRL)\n",
    "        'n_manipulable_vars': 2,\n",
    "        'manipulable_ranges': [\n",
    "            (290, 450),  # Tc [K]\n",
    "            (99, 105)    # F [m¬≥/s]\n",
    "        ],\n",
    "        \n",
    "        # Variable objetivo (optimizada por ORCH)\n",
    "        'n_target_vars': 1,\n",
    "        'target_ranges': [(0.0, 1.0)],           # Cb deseado\n",
    "        'target_setpoints': [0.4],               # Objetivo: 0.4 mol/m¬≥\n",
    "        'target_working_ranges': [(0.0, 2.0)],   # Rango f√≠sico\n",
    "        \n",
    "        'dt_usuario': 1.0,\n",
    "        'max_steps': 50,\n",
    "        \n",
    "        'agent_orchestrator_config': {\n",
    "            'agent_type': 'discrete'\n",
    "        },\n",
    "        \n",
    "        'env_type_config': {\n",
    "            'dt': 1.0,\n",
    "            'control_limits': ((290, 450), (99, 105))\n",
    "        },\n",
    "        \n",
    "        'reward_weights': {\n",
    "            'error': 2.0,\n",
    "            'tiempo': 0.01,\n",
    "            'overshoot': 0.5,\n",
    "            'energy': 0.1\n",
    "        },\n",
    "        'reward_dead_band': 0.02,\n",
    "        \n",
    "        'pid_limits': [\n",
    "            (0.01, 50.0),\n",
    "            (0.0, 5.0),\n",
    "            (0.0, 5.0)\n",
    "        ],\n",
    "        'delta_percent_ctrl': 0.2,\n",
    "        'delta_percent_orch': 0.05,\n",
    "    },\n",
    "    \n",
    "    # ============ AGENTE CTRL (Pre-entrenado) ============\n",
    "    'ctrl_checkpoint_path': str(ctrl_checkpoint_path),  # ‚úÖ Usa el checkpoint reci√©n guardado\n",
    "    'agent_ctrl_config': {\n",
    "        'state_dim': 10,\n",
    "        'action_dim': 7,\n",
    "        'n_vars': 2,\n",
    "        'action_type': 'discrete',\n",
    "        'hidden_dims': (128, 64),\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    },\n",
    "    \n",
    "    # ============ AGENTE ORCH (A entrenar) ============\n",
    "    'agent_orch_config': {\n",
    "        'state_dim': 5,\n",
    "        'action_dim': 3,\n",
    "        'n_vars': 1,\n",
    "        'hidden_dims': (64, 32),\n",
    "        'lr': 0.001,\n",
    "        'gamma': 0.99,\n",
    "        'epsilon_start': 1.0,\n",
    "        'epsilon_min': 0.01,\n",
    "        'epsilon_decay': 0.995,\n",
    "        'batch_size': 32,\n",
    "        'target_update_freq': 100,\n",
    "        'buffer_type': 'simple',\n",
    "        'buffer_size': 10000,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'seed': 42\n",
    "    },\n",
    "    \n",
    "    # ============ ENTRENAMIENTO ============\n",
    "    'n_episodes': 500,\n",
    "    'max_steps_per_episode': 50,\n",
    "    'eval_frequency': 50,\n",
    "    'save_frequency': 9999,\n",
    "    'log_frequency': 10,\n",
    "    'checkpoint_dir': 'checkpoints/cstr_orch',\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n ORCH creada\")\n",
    "print(f\"  Checkpoint CTRL: {config_orch['ctrl_checkpoint_path']}\")\n",
    "print(f\"  Episodios: {config_orch['n_episodes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Test Ambiente Complejo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ambiente\n",
    "test_env_orch = PIDControlEnv_Complex(config_orch['env_config'])\n",
    "sim_cfg = config_orch['env_config']['env_type_config']\n",
    "simulator = CSTRSimulator(**sim_cfg)\n",
    "test_env_orch.proceso.connect_external_process(simulator)\n",
    "print('‚úÖ CSTR conectado al ambiente complejo')\n",
    "\n",
    "obs, info = test_env_orch.reset()\n",
    "print(f\"\\nüìä Observaci√≥n:\")\n",
    "print(f\"  ORCH obs shape: {obs['orch'].shape}\")\n",
    "print(f\"  CTRL obs shape: {obs['ctrl'].shape}\")\n",
    "print(f\"  Cb actual: {info['target_pvs'][0]:.4f} mol/m¬≥\")\n",
    "print(f\"  Cb objetivo: 0.4 mol/m¬≥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Entrenar ORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear trainer ORCH\n",
    "trainer_orch = DQNTrainer(config_orch)\n",
    "\n",
    "# Conectar CSTR\n",
    "sim_cfg = config_orch['env_config']['env_type_config']\n",
    "simulator = CSTRSimulator(**sim_cfg)\n",
    "trainer_orch.env.proceso.connect_external_process(simulator)\n",
    "print('‚úÖ CSTR conectado al trainer ORCH')\n",
    "\n",
    "print(\"\\nüöÄ Entrenando agente ORCH...\\n\")\n",
    "trainer_orch.train()\n",
    "print(\"\\n‚úÖ Entrenamiento ORCH completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Visualizar Resultados ORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä RESULTADOS ORCH\")\n",
    "print_summary(\n",
    "    episode_rewards=trainer_orch.episode_rewards,\n",
    "    episode_energies=trainer_orch.episode_energies,\n",
    "    episode_max_overshoots=trainer_orch.episode_max_overshoots,\n",
    "    best_episode_idx=np.argmax(trainer_orch.episode_rewards)\n",
    ")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Gr√°ficos\n",
    "plotter.plot_training_overview(\n",
    "    episode_rewards=trainer_orch.episode_rewards,\n",
    "    episode_energies=trainer_orch.episode_energies,\n",
    "    episode_max_overshoots=trainer_orch.episode_max_overshoots,\n",
    "    epsilons=trainer_orch.epsilons,\n",
    "    window=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Evaluaci√≥n - Sistema Completo en Acci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modo evaluaci√≥n\n",
    "trainer_orch.agent_orch.epsilon = 0.0\n",
    "\n",
    "# Reset\n",
    "obs, info = trainer_orch.env.reset()\n",
    "print(f\"üìä Condiciones iniciales:\")\n",
    "print(f\"  Cb: {info['target_pvs'][0]:.4f} mol/m¬≥ (objetivo: 0.4)\")\n",
    "print(f\"  Tc: {info['manipulable_pvs'][0]:.2f} K\")\n",
    "print(f\"  F: {info['manipulable_pvs'][1]:.2f} m¬≥/s\")\n",
    "\n",
    "# Ejecutar episodio\n",
    "cb_traj = [info['target_pvs'][0]]\n",
    "tc_traj = [info['manipulable_pvs'][0]]\n",
    "f_traj = [info['manipulable_pvs'][1]]\n",
    "rewards_traj = []\n",
    "\n",
    "done = False\n",
    "step = 0\n",
    "max_steps = 50\n",
    "\n",
    "print(f\"\\nüöÄ Ejecutando episodio...\")\n",
    "print(f\"{'Step':<6} {'Cb':<10} {'Tc':<10} {'F':<10} {'Reward':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "while not done and step < max_steps:\n",
    "    action = trainer_orch.agent_orch.select_action(obs['orch'], training=False)\n",
    "    obs, reward, terminated, truncated, info = trainer_orch.env.step(action)\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    cb_traj.append(info['target_pvs'][0])\n",
    "    tc_traj.append(info['manipulable_pvs'][0])\n",
    "    f_traj.append(info['manipulable_pvs'][1])\n",
    "    rewards_traj.append(reward)\n",
    "    \n",
    "    print(f\"{step:<6} {info['target_pvs'][0]:<10.4f} {info['manipulable_pvs'][0]:<10.2f} \"\n",
    "          f\"{info['manipulable_pvs'][1]:<10.2f} {reward:<10.2f}\")\n",
    "    \n",
    "    step += 1\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\n‚úÖ Episodio completado\")\n",
    "print(f\"  Cb final: {cb_traj[-1]:.4f} mol/m¬≥\")\n",
    "print(f\"  Error final: {abs(cb_traj[-1] - 0.4):.4f}\")\n",
    "print(f\"  Reward total: {sum(rewards_traj):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Visualizaci√≥n Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "steps = np.arange(len(cb_traj))\n",
    "\n",
    "# Cb\n",
    "ax = axes[0]\n",
    "ax.plot(steps, cb_traj, label='Cb', color='#2E86AB', linewidth=2.5, marker='o', markersize=4)\n",
    "ax.axhline(y=0.4, label='Objetivo', color='#C73E1D', linestyle='--', linewidth=2)\n",
    "ax.fill_between(steps, 0.392, 0.408, alpha=0.2, color='#06A77D', label='¬±2%')\n",
    "ax.set_ylabel('Cb [mol/m¬≥]', fontsize=12)\n",
    "ax.set_title('üéØ Concentraci√≥n de B (Variable Objetivo)', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Tc y F\n",
    "ax = axes[1]\n",
    "ax.plot(steps, tc_traj, label='Tc', color='#F18F01', linewidth=2, marker='s', markersize=3)\n",
    "ax.set_ylabel('Tc [K]', fontsize=12, color='#F18F01')\n",
    "ax.tick_params(axis='y', labelcolor='#F18F01')\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(steps, f_traj, label='F', color='#06A77D', linewidth=2, marker='^', markersize=3)\n",
    "ax2.set_ylabel('F [m¬≥/s]', fontsize=12, color='#06A77D')\n",
    "ax2.tick_params(axis='y', labelcolor='#06A77D')\n",
    "ax2.legend(loc='upper right')\n",
    "ax.set_title('üîß Variables Manipuladas (Decididas por ORCH ‚Üí Controladas por CTRL)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Rewards\n",
    "ax = axes[2]\n",
    "ax.plot(range(len(rewards_traj)), rewards_traj, color='#C73E1D', linewidth=2, marker='o', markersize=4)\n",
    "ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Step', fontsize=12)\n",
    "ax.set_ylabel('Reward', fontsize=12)\n",
    "ax.set_title('üí∞ Recompensa', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ Guardar Modelos Finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar ORCH\n",
    "orch_dir = Path(config_orch['checkpoint_dir'])\n",
    "orch_dir.mkdir(exist_ok=True, parents=True)\n",
    "orch_path = orch_dir / 'agent_orch_final.pt'\n",
    "trainer_orch.agent_orch.save(str(orch_path))\n",
    "\n",
    "print(f\"‚úÖ Modelos guardados:\")\n",
    "print(f\"  CTRL: {ctrl_checkpoint_path}\")\n",
    "print(f\"  ORCH: {orch_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üéâ Entrenamiento Completo Finalizado\n",
    "\n",
    "## ‚úÖ Lo que se logr√≥:\n",
    "\n",
    "### **Fase 1 - CTRL**:\n",
    "- ‚úÖ Entrenaste un agente que controla Tc y F del CSTR\n",
    "- ‚úÖ Aprende a ajustar par√°metros PID para alcanzar setpoints\n",
    "\n",
    "### **Fase 2 - ORCH**:\n",
    "- ‚úÖ Entrenaste un agente que optimiza Cb\n",
    "- ‚úÖ Usa CTRL para manipular Tc y F\n",
    "- ‚úÖ Aprende qu√© setpoints dar a CTRL para lograr el objetivo\n",
    "\n",
    "## üöÄ Pr√≥ximos pasos:\n",
    "- Ajustar hiperpar√°metros si es necesario\n",
    "- Probar con diferentes objetivos de Cb\n",
    "- Entrenar por m√°s episodios\n",
    "- Comparar con PID cl√°sico en cascada"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
