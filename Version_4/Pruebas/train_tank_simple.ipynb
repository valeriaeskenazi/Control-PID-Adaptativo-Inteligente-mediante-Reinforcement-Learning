{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Entrenamiento DQN - Tanque Simple\n",
    "\n",
    "Notebook para entrenar agente DQN en control de nivel de tanque.\n",
    "\n",
    "**Objetivo:** Controlar el nivel de un tanque ajustando el caudal de entrada.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports y Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Agregar path del proyecto si es necesario\n",
    "# sys.path.append('../')\n",
    "\n",
    "# Imports del proyecto\n",
    "from simulators.TankSimulator import TankSimulator\n",
    "from environment.SimulationEnv import SimulationPIDEnv\n",
    "from environment.PIDControlEnv_simple import PIDControlEnv_Simple\n",
    "from agents.train_dqn import DQNTrainer, get_simple_config\n",
    "from plotting_utils import TrainingPlotter, plot_quick_summary, print_training_summary\n",
    "\n",
    "print(\"‚úÖ Imports completados\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device disponible: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuraci√≥n del Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n base\n",
    "config = {\n",
    "    # AMBIENTE\n",
    "    'env_config': {\n",
    "        'architecture': 'simple',\n",
    "        'n_manipulable_vars': 1,\n",
    "        'manipulable_ranges': [(0.0, 10.0)],  # Altura del tanque [m]\n",
    "        'manipulable_setpoints': [5.0],  # Setpoint inicial\n",
    "        'dt_usuario': 1.0,\n",
    "        'max_steps': 200,\n",
    "        \n",
    "        'agent_controller_config': {\n",
    "            'agent_type': 'discrete'  # Acciones discretas\n",
    "        },\n",
    "        \n",
    "        # Configuraci√≥n del simulador de tanque\n",
    "        'env_type_config': {\n",
    "            'area': 1.0,          # √Årea del tanque [m¬≤]\n",
    "            'cv': 0.1,            # Coeficiente de descarga\n",
    "            'max_height': 10.0,   # Altura m√°xima [m]\n",
    "            'max_flow_in': 0.5,   # Caudal m√°ximo entrada [m¬≥/s]\n",
    "            'dt': 1.0\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # AGENTE CTRL\n",
    "    'agent_ctrl_config': {\n",
    "        'state_dim': 5,   # pv, sp, error, error_integral, error_derivative\n",
    "        'action_dim': 7,  # 7 acciones discretas (Kp‚Üë, Ki‚Üë, Kd‚Üë, Kp‚Üì, Ki‚Üì, Kd‚Üì, mantener)\n",
    "        'hidden_dims': (128, 64),\n",
    "        'lr': 0.001,\n",
    "        'gamma': 0.99,\n",
    "        'epsilon_start': 1.0,\n",
    "        'epsilon_min': 0.01,\n",
    "        'epsilon_decay': 0.995,\n",
    "        'batch_size': 32,\n",
    "        'target_update_freq': 100,\n",
    "        'buffer_type': 'simple',  # 'simple' o 'priority'\n",
    "        'buffer_size': 10000,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'seed': 42\n",
    "    },\n",
    "    \n",
    "    # ENTRENAMIENTO\n",
    "    'n_episodes': 300,\n",
    "    'max_steps_per_episode': 200,\n",
    "    'eval_frequency': 50,\n",
    "    'save_frequency': 9999,  # No guardar peri√≥dicamente\n",
    "    'log_frequency': 10,\n",
    "    \n",
    "    # LOGGING (comentado)\n",
    "    # 'checkpoint_dir': 'checkpoints/tank_simple',\n",
    "    # 'use_wandb': False,\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n creada\")\n",
    "print(f\"\\nEpisodios de entrenamiento: {config['n_episodes']}\")\n",
    "print(f\"Device: {config['agent_ctrl_config']['device']}\")\n",
    "print(f\"Buffer type: {config['agent_ctrl_config']['buffer_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crear Ambiente de Prueba\n",
    "\n",
    "Primero verificamos que el ambiente funciona correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test del ambiente\n",
    "test_env = PIDControlEnv_Simple(config['env_config'])\n",
    "\n",
    "print(\"üß™ Testing ambiente...\")\n",
    "print(f\"Observation space: {test_env.observation_space}\")\n",
    "print(f\"Action space: {test_env.action_space}\")\n",
    "\n",
    "# Reset\n",
    "obs, info = test_env.reset()\n",
    "print(f\"\\nObservaci√≥n inicial: {obs}\")\n",
    "print(f\"Info inicial: {info}\")\n",
    "\n",
    "# Step aleatorio\n",
    "random_action = test_env.action_space.sample()\n",
    "obs, reward, terminated, truncated, info = test_env.step(random_action)\n",
    "\n",
    "print(f\"\\nDespu√©s de acci√≥n aleatoria {random_action}:\")\n",
    "print(f\"  Reward: {reward:.3f}\")\n",
    "print(f\"  Terminated: {terminated}\")\n",
    "print(f\"  Truncated: {truncated}\")\n",
    "\n",
    "print(\"\\n‚úÖ Ambiente funcionando correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Crear Trainer y Comenzar Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear trainer\n",
    "trainer = DQNTrainer(config)\n",
    "\n",
    "print(\"\\nüéØ Trainer creado\")\n",
    "print(f\"Arquitectura: {trainer.architecture}\")\n",
    "print(f\"Agente CTRL: {type(trainer.agent_ctrl).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTRENAR\n",
    "print(\"\\nüöÄ Iniciando entrenamiento...\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Entrenamiento completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. An√°lisis de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen num√©rico\n",
    "print_training_summary(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico r√°pido\n",
    "plot_quick_summary(\n",
    "    trainer.episode_rewards,\n",
    "    trainer.episode_lengths\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gr√°ficos Detallados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear plotter\n",
    "plotter = TrainingPlotter(save_dir='plots')\n",
    "\n",
    "# M√©tricas de entrenamiento\n",
    "plotter.plot_training_metrics(\n",
    "    episode_rewards=trainer.episode_rewards,\n",
    "    episode_lengths=trainer.episode_lengths,\n",
    "    ctrl_losses=None,  # TODO: guardar losses durante training\n",
    "    ctrl_epsilons=None,  # TODO: guardar epsilons durante training\n",
    "    window=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluar Agente Entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar en un episodio\n",
    "def evaluate_episode(env, agent, setpoint=5.0, render=False):\n",
    "    \"\"\"\n",
    "    Evaluar agente en un episodio completo.\n",
    "    \n",
    "    Returns:\n",
    "        trajectory: Dict con trayectorias de pv, sp, control, error\n",
    "    \"\"\"\n",
    "    state, info = env.reset()\n",
    "    \n",
    "    trajectory = {\n",
    "        'pv': [],\n",
    "        'sp': [],\n",
    "        'control': [],\n",
    "        'error': []\n",
    "    }\n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while not done and steps < 200:\n",
    "        # Seleccionar acci√≥n (SIN exploraci√≥n)\n",
    "        action = agent.select_action(state, training=False)\n",
    "        \n",
    "        # Step\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Guardar trayectoria\n",
    "        trajectory['pv'].append(state[0])  # PV est√° en state[0]\n",
    "        trajectory['sp'].append(state[1])  # SP est√° en state[1]\n",
    "        trajectory['error'].append(state[2])  # Error est√° en state[2]\n",
    "        \n",
    "        # Control output (aproximado desde el ambiente)\n",
    "        # TODO: Necesitar√≠as guardarlo durante el step\n",
    "        trajectory['control'].append(0)  # Placeholder\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        \n",
    "        if render:\n",
    "            print(f\"Step {steps}: PV={state[0]:.2f}, SP={state[1]:.2f}, Error={state[2]:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüìä Episodio completado:\")\n",
    "    print(f\"  Total reward: {total_reward:.2f}\")\n",
    "    print(f\"  Steps: {steps}\")\n",
    "    print(f\"  Final PV: {trajectory['pv'][-1]:.2f}\")\n",
    "    print(f\"  Final Error: {abs(trajectory['error'][-1]):.3f}\")\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "# Ejecutar evaluaci√≥n\n",
    "eval_trajectory = evaluate_episode(\n",
    "    trainer.env,\n",
    "    trainer.agent_ctrl,\n",
    "    setpoint=5.0,\n",
    "    render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar trayectoria\n",
    "plotter.plot_episode_trajectory(\n",
    "    trajectory=eval_trajectory,\n",
    "    setpoint=5.0,\n",
    "    title=\"Agente Entrenado - Control de Nivel\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparar: Agente vs Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar agente random\n",
    "def random_policy_episode(env, setpoint=5.0):\n",
    "    \"\"\"Pol√≠tica aleatoria para comparaci√≥n.\"\"\"\n",
    "    state, info = env.reset()\n",
    "    \n",
    "    trajectory = {\n",
    "        'pv': [],\n",
    "        'sp': [],\n",
    "        'error': [],\n",
    "        'control': []\n",
    "    }\n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while not done and steps < 200:\n",
    "        # Acci√≥n aleatoria\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        trajectory['pv'].append(state[0])\n",
    "        trajectory['sp'].append(state[1])\n",
    "        trajectory['error'].append(state[2])\n",
    "        trajectory['control'].append(0)\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "    \n",
    "    print(f\"Random policy - Reward: {total_reward:.2f}, Steps: {steps}\")\n",
    "    return trajectory\n",
    "\n",
    "random_trajectory = random_policy_episode(trainer.env)\n",
    "\n",
    "# Comparar visualmente\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "# Agente entrenado\n",
    "axes[0].plot(eval_trajectory['pv'], label='Agente DQN', color='blue', linewidth=2)\n",
    "axes[0].axhline(y=5.0, color='red', linestyle='--', label='Setpoint')\n",
    "axes[0].set_ylabel('Height [m]')\n",
    "axes[0].set_title('Agente Entrenado')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Agente random\n",
    "axes[1].plot(random_trajectory['pv'], label='Random Policy', color='orange', linewidth=2)\n",
    "axes[1].axhline(y=5.0, color='red', linestyle='--', label='Setpoint')\n",
    "axes[1].set_xlabel('Step')\n",
    "axes[1].set_ylabel('Height [m]')\n",
    "axes[1].set_title('Pol√≠tica Aleatoria')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ La diferencia es clara: el agente aprendi√≥ a controlar el tanque!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Guardar Modelo (Opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomentar para guardar\n",
    "# save_path = 'models/tank_dqn_final.pt'\n",
    "# Path(save_path).parent.mkdir(exist_ok=True, parents=True)\n",
    "# trainer.agent_ctrl.save(save_path)\n",
    "# print(f\"‚úÖ Modelo guardado en: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cargar Modelo (Opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomentar para cargar modelo guardado\n",
    "# from agents.algorithm_DQN import DQNAgent\n",
    "#\n",
    "# loaded_agent = DQNAgent(\n",
    "#     state_dim=5,\n",
    "#     action_dim=7,\n",
    "#     agent_role='ctrl',\n",
    "#     device='cpu'\n",
    "# )\n",
    "# loaded_agent.load('models/tank_dqn_final.pt')\n",
    "# print(\"‚úÖ Modelo cargado\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
