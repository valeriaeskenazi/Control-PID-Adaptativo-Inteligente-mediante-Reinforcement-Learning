{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Entrenamiento DQN - Tanque Simple\n",
    "\n",
    "Notebook para entrenar agente DQN en control de nivel de tanque.\n",
    "\n",
    "**Objetivo:** Controlar el nivel de un tanque ajustando el caudal de entrada.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports y Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports completados\n",
      "PyTorch version: 2.2.2\n",
      "Device disponible: CPU\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Agregar path del proyecto si es necesario\n",
    "sys.path.append('../')\n",
    "\n",
    "# Imports del proyecto\n",
    "from Environment.Simulation_Env.tanque_simple import TankSimulator\n",
    "from Environment.Simulation_Env.SimulationEnv import SimulationPIDEnv\n",
    "from Environment.PIDControlEnv_simple import PIDControlEnv_Simple\n",
    "from Agente.DQN.train_DQN import DQNTrainer\n",
    "from Aux.Plots import SimplePlotter\n",
    "\n",
    "print(\"Imports completados\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device disponible: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper notebook-only: asegurar que el simulador externo est√© conectado al env/proceso\n",
    "def ensure_external_simulator(env_obj, sim_cfg=None, simulator_name='TankSimulator'):\n",
    "    \"\"\"Si env_obj.proceso.external_process es None, intenta crear y conectar un simulador por defecto.\"\"\"\n",
    "    try:\n",
    "        if getattr(env_obj, 'proceso', None) is not None and getattr(env_obj.proceso, 'external_process', None) is None:\n",
    "            if sim_cfg is None:\n",
    "                sim_cfg = {}\n",
    "            # Por defecto usamos TankSimulator; puedes adaptar para probar otros simuladores\n",
    "            from Environment.Simulation_Env.tanque_simple import TankSimulator\n",
    "            simulator = TankSimulator(**sim_cfg)\n",
    "            env_obj.proceso.connect_external_process(simulator)\n",
    "            print(f\"{simulator_name} conectado autom√°ticamente al proceso\")\n",
    "    except Exception as e:\n",
    "        print('No se pudo conectar simulador autom√°ticamente:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuraci√≥n del Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuraci√≥n creada\n",
      "\n",
      "Episodios de entrenamiento: 300\n",
      "Device: cpu\n",
      "Buffer type: simple\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n base\n",
    "config = {\n",
    "    # AMBIENTE\n",
    "    'env_config': {\n",
    "        'architecture': 'simple',\n",
    "        'n_manipulable_vars': 1,\n",
    "        'manipulable_ranges': [(0.0, 10.0)],  # Altura del tanque [m]\n",
    "        'manipulable_setpoints': [5.0],  # Setpoint inicial\n",
    "        'dt_usuario': 1.0,\n",
    "        'max_steps': 200,\n",
    "        \n",
    "        'agent_controller_config': {\n",
    "            'agent_type': 'discrete'  # Acciones discretas\n",
    "        },\n",
    "        \n",
    "        # Configuraci√≥n del simulador de tanque\n",
    "        'env_type_config': {\n",
    "            'area': 1.0,          # √Årea del tanque [m¬≤]\n",
    "            'cv': 0.1,            # Coeficiente de descarga\n",
    "            'max_height': 10.0,   # Altura m√°xima [m]\n",
    "            'max_flow_in': 0.5,   # Caudal m√°ximo entrada [m¬≥/s]\n",
    "            'dt': 1.0\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # AGENTE CTRL\n",
    "    'agent_ctrl_config': {\n",
    "        'state_dim': 5,   # pv, sp, error, error_integral, error_derivative\n",
    "        'action_dim': 7,  # 7 acciones discretas (Kp‚Üë, Ki‚Üë, Kd‚Üë, Kp‚Üì, Ki‚Üì, Kd‚Üì, mantener)\n",
    "        'hidden_dims': (128, 64),\n",
    "        'lr': 0.001,\n",
    "        'gamma': 0.99,\n",
    "        'epsilon_start': 1.0,\n",
    "        'epsilon_min': 0.01,\n",
    "        'epsilon_decay': 0.995,\n",
    "        'batch_size': 32,\n",
    "        'target_update_freq': 100,\n",
    "        'buffer_type': 'simple',  # 'simple' o 'priority'\n",
    "        'buffer_size': 10000,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'seed': 42\n",
    "    },\n",
    "    \n",
    "    # ENTRENAMIENTO\n",
    "    'n_episodes': 300,\n",
    "    'max_steps_per_episode': 200,\n",
    "    'eval_frequency': 50,\n",
    "    'save_frequency': 9999,  # No guardar peri√≥dicamente\n",
    "    'log_frequency': 10,\n",
    "    \n",
    "    # LOGGING (comentado)\n",
    "    'checkpoint_dir': 'checkpoints/tank_simple',\n",
    "    # 'use_wandb': False,\n",
    "}\n",
    "\n",
    "print(\"Configuraci√≥n creada\")\n",
    "print(f\"\\nEpisodios de entrenamiento: {config['n_episodes']}\")\n",
    "print(f\"Device: {config['agent_ctrl_config']['device']}\")\n",
    "print(f\"Buffer type: {config['agent_ctrl_config']['buffer_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crear Ambiente de Prueba\n",
    "\n",
    "Primero verificamos que el ambiente funciona correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(-inf, inf, (5,), float32)\n",
      "Action space: MultiDiscrete([7])\n",
      "TankSimulator conectado autom√°ticamente al proceso\n",
      "\n",
      "Observaci√≥n inicial: [8.037754 6.567536 0.       0.       0.      ]\n",
      "Info inicial: {'trajectory_manipulable': [[]], 'energy': 0.0, 'overshoot_manipulable': [0.0], 'accumulated_error_manipulable': [0.0]}\n",
      "\n",
      "Despu√©s de acci√≥n aleatoria [4]:\n",
      "  Observaci√≥n: [6.5075254  6.567536   0.06001076 0.06001076 0.06001076]\n",
      "  Reward: -49.222\n",
      "  Terminated: True\n",
      "  Truncated: False\n",
      "  Info: {'trajectory_manipulable': [[8.037754057431851, 0.0, 0.5055938200054554, 0.9312263250402749, 1.3318063233144881, 1.709438325595574, 2.0855727148033716, 2.4386185729970316, 2.778534517256791, 3.1217914698394456, 3.4333014308184464, 3.7610822284980316, 4.0715927135631755, 4.3512728117629225, 4.662265234270804, 4.939075918388991, 5.216771901531183, 5.476326516801749, 5.742420369114229, 6.012622190643273, 6.2649275323672935, 6.5075252489369415]], 'energy': 131.48097399161074, 'overshoot_manipulable': [22.386143743138177], 'accumulated_error_manipulable': [64.04670802407878]}\n",
      "\n",
      "Ambiente funcionando correctamente\n",
      "\n",
      "Despu√©s de acci√≥n aleatoria [4]:\n",
      "  Observaci√≥n: [6.5075254  6.567536   0.06001076 0.06001076 0.06001076]\n",
      "  Reward: -49.222\n",
      "  Terminated: True\n",
      "  Truncated: False\n",
      "  Info: {'trajectory_manipulable': [[8.037754057431851, 0.0, 0.5055938200054554, 0.9312263250402749, 1.3318063233144881, 1.709438325595574, 2.0855727148033716, 2.4386185729970316, 2.778534517256791, 3.1217914698394456, 3.4333014308184464, 3.7610822284980316, 4.0715927135631755, 4.3512728117629225, 4.662265234270804, 4.939075918388991, 5.216771901531183, 5.476326516801749, 5.742420369114229, 6.012622190643273, 6.2649275323672935, 6.5075252489369415]], 'energy': 131.48097399161074, 'overshoot_manipulable': [22.386143743138177], 'accumulated_error_manipulable': [64.04670802407878]}\n",
      "\n",
      "Ambiente funcionando correctamente\n"
     ]
    }
   ],
   "source": [
    "# Test del ambiente (auto-conexi√≥n del simulador si hace falta)\n",
    "import traceback\n",
    "try:\n",
    "    test_env = PIDControlEnv_Simple(config['env_config'])\n",
    "    print(f\"Observation space: {test_env.observation_space}\")\n",
    "    print(f\"Action space: {test_env.action_space}\")\n",
    "\n",
    "    # Si el proceso no tiene external_process, crear y conectar TankSimulator\n",
    "    try:\n",
    "        from Environment.Simulation_Env.tanque_simple import TankSimulator\n",
    "        if getattr(test_env, 'proceso', None) is not None and getattr(test_env.proceso, 'external_process', None) is None:\n",
    "            sim_cfg = config.get('env_config', {}).get('env_type_config', {}) or {}\n",
    "            simulator = TankSimulator(**sim_cfg)\n",
    "            test_env.proceso.connect_external_process(simulator)\n",
    "            print('TankSimulator conectado autom√°ticamente al proceso')\n",
    "    except Exception as e:\n",
    "        print('No se pudo conectar autom√°ticamente TankSimulator:', e)\n",
    "\n",
    "    # Reset\n",
    "    obs, info = test_env.reset()\n",
    "    print(f\"\\nObservaci√≥n inicial: {obs}\")\n",
    "    print(f\"Info inicial: {info}\")\n",
    "\n",
    "    # Step aleatorio\n",
    "    random_action = test_env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = test_env.step(random_action)\n",
    "\n",
    "    print(f\"\\nDespu√©s de acci√≥n aleatoria {random_action}:\")\n",
    "    print(f\"  Observaci√≥n: {obs}\")\n",
    "    print(f\"  Reward: {reward:.3f}\")\n",
    "    print(f\"  Terminated: {terminated}\")\n",
    "    print(f\"  Truncated: {truncated}\")\n",
    "    print(f\"  Info: {info}\")\n",
    "\n",
    "    print(\"\\nAmbiente funcionando correctamente\")\n",
    "except Exception:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TankSimulator conectado autom√°ticamente al proceso\n",
      "Testing ambiente\n",
      "Observation space: Box(-inf, inf, (5,), float32)\n",
      "Action space: MultiDiscrete([7])\n",
      "\n",
      "Observaci√≥n inicial: [6.447276  6.6048307 0.        0.        0.       ]\n",
      "Info inicial: {'trajectory_manipulable': [[]], 'energy': 0.0, 'overshoot_manipulable': [0.0], 'accumulated_error_manipulable': [0.0]}\n",
      "\n",
      "Despu√©s de acci√≥n aleatoria [5]:\n",
      "  Observaci√≥n: [ 6.6488743   6.6048307  -0.04404362 -0.04404362 -0.04404362]\n",
      "  Reward: -29.917\n",
      "  Terminated: True\n",
      "  Truncated: False\n",
      "  Info: {'trajectory_manipulable': [[6.447276331484442, 0.28779008268398587, 0.7463052014629881, 1.150799314652018, 1.5538642842726829, 1.9309850576021734, 2.274038422150716, 2.6239887615502884, 2.976181974155476, 3.2995279059137013, 3.624647225075792, 3.933925375775272, 4.233082888238015, 4.516136621773732, 4.815599024329186, 5.095506176988669, 5.355770600672621, 5.629082757523946, 5.900096935155979, 6.161546005588785, 6.412881969322014, 6.648874506664781]], 'energy': 143.7119192720326, 'overshoot_manipulable': [0.6668394603666772], 'accumulated_error_manipulable': [59.77645935055278]}\n",
      "\n",
      "Ambiente funcionando correctamente\n"
     ]
    }
   ],
   "source": [
    "# Test del ambiente\n",
    "test_env = PIDControlEnv_Simple(config['env_config'])\n",
    "sim_cfg = config.get('env_config', {}).get('env_type_config', {}) or {}\n",
    "simulator = TankSimulator(**sim_cfg)\n",
    "test_env.proceso.connect_external_process(simulator)\n",
    "print('TankSimulator conectado autom√°ticamente al proceso')\n",
    "print(\"Testing ambiente\")\n",
    "print(f\"Observation space: {test_env.observation_space}\")\n",
    "print(f\"Action space: {test_env.action_space}\")\n",
    "\n",
    "# Reset\n",
    "obs, info = test_env.reset()\n",
    "print(f\"\\nObservaci√≥n inicial: {obs}\")\n",
    "print(f\"Info inicial: {info}\")\n",
    "\n",
    "# Step aleatorio\n",
    "random_action = test_env.action_space.sample()\n",
    "obs, reward, terminated, truncated, info = test_env.step(random_action)\n",
    "\n",
    "print(f\"\\nDespu√©s de acci√≥n aleatoria {random_action}:\")\n",
    "print(f\"  Observaci√≥n: {obs}\")\n",
    "print(f\"  Reward: {reward:.3f}\")\n",
    "print(f\"  Terminated: {terminated}\")\n",
    "print(f\"  Truncated: {truncated}\")\n",
    "print(f\"  Info: {info}\")\n",
    "\n",
    "print(\"\\nAmbiente funcionando correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Crear Trainer y Comenzar Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Trainer creado\n",
      "Arquitectura: simple\n",
      "Agente CTRL: DQNAgent\n"
     ]
    }
   ],
   "source": [
    "# Crear trainer\n",
    "trainer = DQNTrainer(config)\n",
    "\n",
    "print(\"\\n Trainer creado\")\n",
    "print(f\"Arquitectura: {trainer.architecture}\")\n",
    "print(f\"Agente CTRL: {type(trainer.agent_ctrl).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Iniciando entrenamiento...\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m sim_cfg \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menv_config\u001b[39m\u001b[38;5;124m'\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menv_type_config\u001b[39m\u001b[38;5;124m'\u001b[39m, {}) \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m      6\u001b[0m ensure_external_simulator(trainer\u001b[38;5;241m.\u001b[39menv, sim_cfg\u001b[38;5;241m=\u001b[39msim_cfg)\n\u001b[0;32m----> 7\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Entrenamiento completado\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/valeria/MASTER/TESIS/PID_Agent/Version_4/Pruebas/../Agente/DQN/train_DQN.py:133\u001b[0m, in \u001b[0;36mDQNTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_episodes):\n\u001b[0;32m--> 133\u001b[0m         episode_reward, episode_length, episode_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_episode(episode, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;66;03m# Guardar estad√≠sticas\u001b[39;00m\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_rewards\u001b[38;5;241m.\u001b[39mappend(episode_reward)\n",
      "File \u001b[0;32m~/Documents/valeria/MASTER/TESIS/PID_Agent/Version_4/Pruebas/../Agente/DQN/train_DQN.py:198\u001b[0m, in \u001b[0;36mDQNTrainer._run_episode\u001b[0;34m(self, episode, training)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_ctrl\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39madd(experience)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Actualizar agente\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_ctrl\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metrics:\n\u001b[1;32m    200\u001b[0m     ctrl_losses\u001b[38;5;241m.\u001b[39mappend(metrics\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/Documents/valeria/MASTER/TESIS/PID_Agent/Version_4/Pruebas/../Agente/DQN/algorithm_DQN.py:129\u001b[0m, in \u001b[0;36mDQNAgent.update\u001b[0;34m(self, batch_data)\u001b[0m\n\u001b[1;32m    126\u001b[0m dones \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdones\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Calcular Q-values actuales (CON gradientes)\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m current_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_network(states)\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Calcular Q-values objetivo (SIN gradientes)\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "# ENTRENAR\n",
    "print(\"\\n Iniciando entrenamiento...\\n\")\n",
    "\n",
    "# Asegurar que el simulador externo est√© conectado (solo notebook)\n",
    "sim_cfg = config.get('env_config', {}).get('env_type_config', {}) or {}\n",
    "ensure_external_simulator(trainer.env, sim_cfg=sim_cfg)\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n Entrenamiento completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. An√°lisis de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen num√©rico\n",
    "print_training_summary(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico r√°pido\n",
    "plot_quick_summary(\n",
    "    trainer.episode_rewards,\n",
    "    trainer.episode_lengths\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gr√°ficos Detallados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear plotter\n",
    "plotter = TrainingPlotter(save_dir='plots')\n",
    "\n",
    "# M√©tricas de entrenamiento\n",
    "plotter.plot_training_metrics(\n",
    "    episode_rewards=trainer.episode_rewards,\n",
    "    episode_lengths=trainer.episode_lengths,\n",
    "    ctrl_losses=None,  # TODO: guardar losses durante training\n",
    "    ctrl_epsilons=None,  # TODO: guardar epsilons durante training\n",
    "    window=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluar Agente Entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Episodio completado:\n",
      "  Total reward: -240.32\n",
      "  Steps: 1\n",
      "  Final PV: 8.52\n",
      "  Final Error: 0.000\n"
     ]
    }
   ],
   "source": [
    "# Evaluar en un episodio\n",
    "def evaluate_episode(env, agent, setpoint=5.0, render=False):\n",
    "    \"\"\"\n",
    "    Evaluar agente en un episodio completo.\n",
    "    \n",
    "    Returns:\n",
    "        trajectory: Dict con trayectorias de pv, sp, control, error\n",
    "    \"\"\"\n",
    "    state, info = env.reset()\n",
    "    \n",
    "    trajectory = {\n",
    "        'pv': [],\n",
    "        'sp': [],\n",
    "        'control': [],\n",
    "        'error': []\n",
    "    }\n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while not done and steps < 200:\n",
    "        # Seleccionar acci√≥n (SIN exploraci√≥n)\n",
    "        action = agent.select_action(state, training=False)\n",
    "        \n",
    "        # Step\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Guardar trayectoria\n",
    "        trajectory['pv'].append(state[0])  # PV est√° en state[0]\n",
    "        trajectory['sp'].append(state[1])  # SP est√° en state[1]\n",
    "        trajectory['error'].append(state[2])  # Error est√° en state[2]\n",
    "        \n",
    "        # Control output (aproximado desde el ambiente)\n",
    "        # TODO: Necesitar√≠as guardarlo durante el step\n",
    "        trajectory['control'].append(0)  # Placeholder\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        \n",
    "        if render:\n",
    "            print(f\"Step {steps}: PV={state[0]:.2f}, SP={state[1]:.2f}, Error={state[2]:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüìä Episodio completado:\")\n",
    "    print(f\"  Total reward: {total_reward:.2f}\")\n",
    "    print(f\"  Steps: {steps}\")\n",
    "    print(f\"  Final PV: {trajectory['pv'][-1]:.2f}\")\n",
    "    print(f\"  Final Error: {abs(trajectory['error'][-1]):.3f}\")\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "# Ejecutar evaluaci√≥n\n",
    "eval_trajectory = evaluate_episode(\n",
    "    trainer.env,\n",
    "    trainer.agent_ctrl,\n",
    "    setpoint=5.0,\n",
    "    render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar trayectoria\n",
    "plotter.plot_episode_trajectory(\n",
    "    trajectory=eval_trajectory,\n",
    "    setpoint=5.0,\n",
    "    title=\"Agente Entrenado - Control de Nivel\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparar: Agente vs Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar agente random\n",
    "def random_policy_episode(env, setpoint=5.0):\n",
    "    \"\"\"Pol√≠tica aleatoria para comparaci√≥n.\"\"\"\n",
    "    state, info = env.reset()\n",
    "    \n",
    "    trajectory = {\n",
    "        'pv': [],\n",
    "        'sp': [],\n",
    "        'error': [],\n",
    "        'control': []\n",
    "    }\n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while not done and steps < 200:\n",
    "        # Acci√≥n aleatoria\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        trajectory['pv'].append(state[0])\n",
    "        trajectory['sp'].append(state[1])\n",
    "        trajectory['error'].append(state[2])\n",
    "        trajectory['control'].append(0)\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "    \n",
    "    print(f\"Random policy - Reward: {total_reward:.2f}, Steps: {steps}\")\n",
    "    return trajectory\n",
    "\n",
    "random_trajectory = random_policy_episode(trainer.env)\n",
    "\n",
    "# Comparar visualmente\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "# Agente entrenado\n",
    "axes[0].plot(eval_trajectory['pv'], label='Agente DQN', color='blue', linewidth=2)\n",
    "axes[0].axhline(y=5.0, color='red', linestyle='--', label='Setpoint')\n",
    "axes[0].set_ylabel('Height [m]')\n",
    "axes[0].set_title('Agente Entrenado')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Agente random\n",
    "axes[1].plot(random_trajectory['pv'], label='Random Policy', color='orange', linewidth=2)\n",
    "axes[1].axhline(y=5.0, color='red', linestyle='--', label='Setpoint')\n",
    "axes[1].set_xlabel('Step')\n",
    "axes[1].set_ylabel('Height [m]')\n",
    "axes[1].set_title('Pol√≠tica Aleatoria')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ La diferencia es clara: el agente aprendi√≥ a controlar el tanque!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Guardar Modelo (Opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomentar para guardar\n",
    "# save_path = 'models/tank_dqn_final.pt'\n",
    "# Path(save_path).parent.mkdir(exist_ok=True, parents=True)\n",
    "# trainer.agent_ctrl.save(save_path)\n",
    "# print(f\"‚úÖ Modelo guardado en: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cargar Modelo (Opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomentar para cargar modelo guardado\n",
    "# from agents.algorithm_DQN import DQNAgent\n",
    "#\n",
    "# loaded_agent = DQNAgent(\n",
    "#     state_dim=5,\n",
    "#     action_dim=7,\n",
    "#     agent_role='ctrl',\n",
    "#     device='cpu'\n",
    "# )\n",
    "# loaded_agent.load('models/tank_dqn_final.pt')\n",
    "# print(\"‚úÖ Modelo cargado\")\n",
    "\n",
    "# Notebook-only helper: asegurar shapes correctas para acciones en batches\n",
    "# Coloca esta celda justo antes de iniciar el entrenamiento (antes de trainer.train())\n",
    "def normalize_batch_shapes_for_dqn(batch):\n",
    "    \"\"\"Ajusta shapes comunes devueltas por los buffers para que sean compatibles con gather.\n",
    "\n",
    "    - actions: LongTensor con shape [batch]\n",
    "    - rewards, dones: FloatTensor/BoolTensor con shape [batch]\n",
    "    - states/next_states: FloatTensor con shape [batch, state_dim]\n",
    "    \"\"\"\n",
    "    if 'actions' in batch:\n",
    "        actions = batch['actions']\n",
    "        # Si actions viene con shape [batch, n_vars] y n_vars>1, esto indica acciones m√∫ltiples por variable.\n",
    "        # DQN actual espera un action por muestra (discrete scalar). Convertimos a shape [batch] si es necesario.\n",
    "        if actions.dim() > 1:\n",
    "            # Si la segunda dim es 1, squeeze\n",
    "            if actions.shape[1] == 1:\n",
    "                actions = actions.squeeze(1)\n",
    "            else:\n",
    "                # Si hay m√∫ltiples acciones por muestra (p.ej. por variable), colapsar tomando la primera\n",
    "                # (usuario puede adaptar: por ejemplo, convertir a una acci√≥n compuesta)\n",
    "                actions = actions[:, 0]\n",
    "        # Asegurar tipo LongTensor si corresponde\n",
    "        if not torch.is_tensor(actions):\n",
    "            actions = torch.tensor(actions, dtype=torch.long, device=batch['states'].device)\n",
    "        elif not actions.dtype in (torch.int64,):\n",
    "            actions = actions.long()\n",
    "\n",
    "        batch['actions'] = actions\n",
    "\n",
    "    # Rewards\n",
    "    if 'rewards' in batch and not torch.is_tensor(batch['rewards']):\n",
    "        batch['rewards'] = torch.tensor(batch['rewards'], dtype=torch.float32, device=batch['states'].device)\n",
    "\n",
    "    # Dones\n",
    "    if 'dones' in batch and not torch.is_tensor(batch['dones']):\n",
    "        batch['dones'] = torch.tensor(batch['dones'], dtype=torch.bool, device=batch['states'].device)\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "# Uso sugerido:\n",
    "# batch = trainer.agent_ctrl.memory.sample(batch_size)\n",
    "# batch = normalize_batch_shapes_for_dqn(batch)\n",
    "# Luego pasar batch['states'], batch['actions'], ... a update() si se quisiera llamar manualmente.\n",
    "\n",
    "# Notebook-only: monkey-patch para normalizar batches devueltos por memory.sample\n",
    "# Ejecuta esta celda antes de llamar a trainer.train()\n",
    "import types\n",
    "\n",
    "\n",
    "def _normalize_batch(batch):\n",
    "    # Reusar la funci√≥n ya definida en la notebook si existe\n",
    "    try:\n",
    "        normalized = normalize_batch_shapes_for_dqn(batch)\n",
    "    except NameError:\n",
    "        # Fallback local\n",
    "        import torch\n",
    "        actions = batch.get('actions', None)\n",
    "        if actions is not None:\n",
    "            if hasattr(actions, 'dim') and actions.dim() > 1:\n",
    "                if actions.shape[1] == 1:\n",
    "                    actions = actions.squeeze(1)\n",
    "                else:\n",
    "                    actions = actions[:, 0]\n",
    "            if not torch.is_tensor(actions):\n",
    "                actions = torch.tensor(actions, dtype=torch.long, device=batch['states'].device)\n",
    "            elif not actions.dtype in (torch.int64,):\n",
    "                actions = actions.long()\n",
    "            batch['actions'] = actions\n",
    "        if 'rewards' in batch and not torch.is_tensor(batch['rewards']):\n",
    "            batch['rewards'] = torch.tensor(batch['rewards'], dtype=torch.float32, device=batch['states'].device)\n",
    "        if 'dones' in batch and not torch.is_tensor(batch['dones']):\n",
    "            batch['dones'] = torch.tensor(batch['dones'], dtype=torch.bool, device=batch['states'].device)\n",
    "        normalized = batch\n",
    "    return normalized\n",
    "\n",
    "\n",
    "# Wrap sample method of a buffer instance\n",
    "def patch_buffer_sample(buffer):\n",
    "    if hasattr(buffer, '_sample_patched') and buffer._sample_patched:\n",
    "        return\n",
    "    original_sample = buffer.sample\n",
    "\n",
    "    def wrapped_sample(batch_size):\n",
    "        batch = original_sample(batch_size)\n",
    "        return _normalize_batch(batch)\n",
    "\n",
    "    buffer.sample = types.MethodType(wrapped_sample, buffer)\n",
    "    buffer._sample_patched = True\n",
    "    print(f'Patched sample() on buffer: {type(buffer).__name__}')\n",
    "\n",
    "\n",
    "# Aplicar patch al buffer del trainer\n",
    "try:\n",
    "    patch_buffer_sample(trainer.agent_ctrl.memory)\n",
    "    if getattr(trainer, 'agent_orch', None) is not None:\n",
    "        patch_buffer_sample(trainer.agent_orch.memory)\n",
    "except Exception as e:\n",
    "    print('No se pudo aplicar monkey-patch en buffers:', e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
