{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ Exploraci√≥n: DQN para Control de Tanque\n",
    "\n",
    "Este notebook permite explorar y probar el agente DQN de forma interactiva antes del entrenamiento completo.\n",
    "\n",
    "## üéØ Objetivos:\n",
    "- Verificar que el simulador funciona correctamente\n",
    "- Probar el agente DQN con pocos episodios\n",
    "- Ajustar hiperpar√°metros interactivamente\n",
    "- Visualizar resultados inmediatamente\n",
    "- Debugging paso a paso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Imports y Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Agregar paths para importar m√≥dulos del proyecto\n",
    "sys.path.append('../..')\n",
    "\n",
    "from simulations.tanque_nivel.tanque_simulator import TankLevelSimulator\n",
    "from agent_valeria.DQN.dqn_agent import DQN_Agent\n",
    "\n",
    "# Configurar matplotlib para notebooks\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úÖ Imports completados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Paso 1: Probar el Simulador de Tanque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear simulador\n",
    "tank = TankLevelSimulator(\n",
    "    tank_area=2.0,      # m¬≤\n",
    "    max_height=5.0,     # m\n",
    "    max_inflow=10.0,    # L/s\n",
    "    dt=1.0,            # segundos\n",
    "    noise_level=0.02    # 2% ruido\n",
    ")\n",
    "\n",
    "print(\"üèóÔ∏è Simulador creado\")\n",
    "print(f\"√Årea del tanque: {tank.tank_area} m¬≤\")\n",
    "print(f\"Altura m√°xima: {tank.max_height} m\")\n",
    "print(f\"Caudal m√°ximo: {tank.max_inflow} L/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar simulador con control manual\n",
    "print(\"üß™ Prueba r√°pida del simulador...\")\n",
    "\n",
    "# Reiniciar con condiciones conocidas (NUEVA INTERFACE GYM)\n",
    "observation, info = tank.reset(options={'initial_level': 1.0, 'setpoint': 3.0})\n",
    "print(f\"Observaci√≥n inicial: {observation}\")\n",
    "print(f\"Info inicial: {info}\")\n",
    "print(f\"  Nivel: {observation[0]:.2f}m\")\n",
    "print(f\"  Setpoint: {observation[1]:.2f}m\")\n",
    "print(f\"  Error: {observation[2]:.2f}m\")\n",
    "\n",
    "# Simular algunos pasos con control simple\n",
    "steps_data = []\n",
    "for step in range(20):\n",
    "    # Control proporcional simple: u = base + K*error\n",
    "    error = observation[2]\n",
    "    control = 4.0 + 2.0 * error  # Base 4 L/s + proporcional\n",
    "    control = np.clip(control, 0, tank.max_inflow)\n",
    "    \n",
    "    # Ejecutar paso (NUEVA INTERFACE GYM)\n",
    "    next_observation, reward, terminated, truncated, info = tank.step(control)\n",
    "    \n",
    "    # Guardar datos para gr√°fico\n",
    "    steps_data.append({\n",
    "        'step': step,\n",
    "        'level': observation[0],\n",
    "        'setpoint': observation[1],\n",
    "        'error': observation[2],\n",
    "        'control': control,\n",
    "        'reward': reward\n",
    "    })\n",
    "    \n",
    "    observation = next_observation\n",
    "    \n",
    "    if step % 5 == 0:\n",
    "        print(f\"  Step {step:2d}: Nivel={observation[0]:.2f}m, Error={observation[2]:.3f}m, \"\n",
    "              f\"Control={control:.1f}L/s, Reward={reward:.3f}\")\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(\"\\n‚úÖ Simulador funcionando correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultados del test\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(steps_data)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Nivel vs Setpoint\n",
    "ax1.plot(df['step'], df['level'], 'b-', linewidth=2, label='Nivel Real')\n",
    "ax1.plot(df['step'], df['setpoint'], 'r--', linewidth=2, label='Setpoint')\n",
    "ax1.set_title('Nivel del Tanque')\n",
    "ax1.set_ylabel('Nivel (m)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Error\n",
    "ax2.plot(df['step'], df['error'], 'g-', linewidth=2)\n",
    "ax2.set_title('Error de Control')\n",
    "ax2.set_ylabel('Error (m)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Se√±al de Control\n",
    "ax3.plot(df['step'], df['control'], 'm-', linewidth=2)\n",
    "ax3.set_title('Se√±al de Control')\n",
    "ax3.set_xlabel('Step')\n",
    "ax3.set_ylabel('Caudal (L/s)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Recompensa\n",
    "ax4.plot(df['step'], df['reward'], 'c-', linewidth=2)\n",
    "ax4.set_title('Recompensa')\n",
    "ax4.set_xlabel('Step')\n",
    "ax4.set_ylabel('Reward')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Prueba del Simulador de Tanque - Control Proporcional Simple', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Recompensa promedio: {df['reward'].mean():.3f}\")\n",
    "print(f\"üìä Error final: {df['error'].iloc[-1]:.3f}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Paso 2: Crear y Probar Agente DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear agente DQN\n",
    "agent = DQN_Agent(\n",
    "    state_dim=6,           # [level, setpoint, error, prev_error, integral, derivative]\n",
    "    lr=0.001,             # Learning rate\n",
    "    gamma=0.99,           # Discount factor\n",
    "    epsilon_start=1.0,    # Exploraci√≥n inicial\n",
    "    epsilon_end=0.01,     # Exploraci√≥n final\n",
    "    epsilon_decay=0.995,  # Decaimiento\n",
    "    memory_size=1000,     # Buffer peque√±o para pruebas\n",
    "    batch_size=32,\n",
    "    target_update_freq=50, # Actualizar target m√°s frecuente\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "print(\"üß† Agente DQN creado\")\n",
    "print(f\"  Acciones discretas: {agent.n_actions}\")\n",
    "print(f\"  Epsilon inicial: {agent.get_epsilon():.3f}\")\n",
    "print(f\"  Memoria disponible: {len(agent.memory)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar selecci√≥n de acciones sin entrenar\n",
    "print(\"üéØ Probando selecci√≥n de acciones (agente no entrenado)...\")\n",
    "\n",
    "# Estado de prueba\n",
    "test_observation = np.array([2.0, 3.0, 1.0, 0.8, 5.0, 0.2])  # [level, setpoint, error, prev_error, integral, derivative]\n",
    "print(f\"Observaci√≥n de prueba: {test_observation}\")\n",
    "\n",
    "# Probar varias acciones\n",
    "for i in range(5):\n",
    "    pid_params = agent.select_action(test_observation, training=True)\n",
    "    action_idx = agent.get_last_action_index()\n",
    "    \n",
    "    print(f\"  Acci√≥n {i+1}: √çndice={action_idx:2d}, PID=[{pid_params[0]:.2f}, {pid_params[1]:.2f}, {pid_params[2]:.2f}]\")\n",
    "\n",
    "print(f\"\\nüé≤ Epsilon actual: {agent.get_epsilon():.3f} (alta exploraci√≥n = acciones aleatorias)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÇÔ∏è Paso 3: Entrenamiento Corto (Exploraci√≥n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_short_episode(agent, simulator, max_steps=50, verbose=True):\n",
    "    \"\"\"\n",
    "    Entrenar un episodio corto para pruebas\n",
    "    \"\"\"\n",
    "    # Reiniciar simulador (NUEVA INTERFACE GYM)\n",
    "    setpoint = np.random.uniform(2.0, 4.0)\n",
    "    observation, info = simulator.reset(options={'setpoint': setpoint})\n",
    "    \n",
    "    episode_data = []\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Acci√≥n del agente\n",
    "        pid_params = agent.select_action(observation, training=True)\n",
    "        \n",
    "        # Control PID\n",
    "        kp, ki, kd = pid_params\n",
    "        error = observation[2]\n",
    "        integral = observation[4]\n",
    "        derivative = observation[5]\n",
    "        \n",
    "        control_signal = kp * error + ki * integral + kd * derivative\n",
    "        control_signal = 4.0 + control_signal  # Base flow\n",
    "        control_signal = np.clip(control_signal, 0.0, simulator.max_inflow)\n",
    "        \n",
    "        # Ejecutar paso (NUEVA INTERFACE GYM)\n",
    "        next_observation, reward, terminated, truncated, info = simulator.step(control_signal)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Almacenar experiencia\n",
    "        agent.store_experience(observation, pid_params, reward, next_observation, done)\n",
    "        \n",
    "        # Actualizar agente\n",
    "        metrics = agent.update()\n",
    "        \n",
    "        # Guardar datos\n",
    "        episode_data.append({\n",
    "            'step': step,\n",
    "            'level': observation[0],\n",
    "            'setpoint': observation[1],\n",
    "            'error': observation[2],\n",
    "            'kp': kp,\n",
    "            'ki': ki,\n",
    "            'kd': kd,\n",
    "            'control': control_signal,\n",
    "            'reward': reward,\n",
    "            'q_loss': metrics.get('q_loss', 0),\n",
    "            'epsilon': agent.get_epsilon()\n",
    "        })\n",
    "        \n",
    "        total_reward += reward\n",
    "        observation = next_observation\n",
    "        \n",
    "        if verbose and step % 10 == 0:\n",
    "            print(f\"  Step {step:2d}: Nivel={observation[0]:.2f}m, Error={error:.3f}m, \"\n",
    "                  f\"PID=[{kp:.1f},{ki:.1f},{kd:.1f}], Reward={reward:.3f}\")\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return episode_data, total_reward\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de entrenamiento actualizada con interface Gym\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar un episodio de prueba\n",
    "print(\"üèÉ‚Äç‚ôÇÔ∏è Entrenando episodio de prueba (50 steps)...\")\n",
    "print(f\"Memoria inicial: {len(agent.memory)} experiencias\")\n",
    "print(f\"Epsilon inicial: {agent.get_epsilon():.3f}\")\n",
    "\n",
    "episode_data, total_reward = train_short_episode(agent, tank, max_steps=50, verbose=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Episodio completado:\")\n",
    "print(f\"  Recompensa total: {total_reward:.2f}\")\n",
    "print(f\"  Memoria final: {len(agent.memory)} experiencias\")\n",
    "print(f\"  Epsilon final: {agent.get_epsilon():.3f}\")\n",
    "print(f\"  Steps ejecutados: {len(episode_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultados del episodio\n",
    "df_episode = pd.DataFrame(episode_data)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Nivel vs Setpoint\n",
    "ax1.plot(df_episode['step'], df_episode['level'], 'b-', linewidth=2, label='Nivel')\n",
    "ax1.plot(df_episode['step'], df_episode['setpoint'], 'r--', linewidth=2, label='Setpoint')\n",
    "ax1.fill_between(df_episode['step'], df_episode['level'], df_episode['setpoint'], alpha=0.3)\n",
    "ax1.set_title('Control de Nivel - DQN')\n",
    "ax1.set_ylabel('Nivel (m)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Par√°metros PID seleccionados\n",
    "ax2.plot(df_episode['step'], df_episode['kp'], 'r-', label='Kp', alpha=0.7)\n",
    "ax2.plot(df_episode['step'], df_episode['ki'], 'g-', label='Ki', alpha=0.7)\n",
    "ax2.plot(df_episode['step'], df_episode['kd'], 'b-', label='Kd', alpha=0.7)\n",
    "ax2.set_title('Par√°metros PID Seleccionados')\n",
    "ax2.set_ylabel('Valor')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Recompensa y Epsilon\n",
    "ax3_twin = ax3.twinx()\n",
    "line1 = ax3.plot(df_episode['step'], df_episode['reward'], 'c-', linewidth=2, label='Reward')\n",
    "line2 = ax3_twin.plot(df_episode['step'], df_episode['epsilon'], 'm--', linewidth=2, label='Epsilon')\n",
    "ax3.set_title('Recompensa y Exploraci√≥n')\n",
    "ax3.set_xlabel('Step')\n",
    "ax3.set_ylabel('Reward', color='c')\n",
    "ax3_twin.set_ylabel('Epsilon', color='m')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Loss (solo si hay datos)\n",
    "q_losses = [x for x in df_episode['q_loss'] if x > 0]\n",
    "if q_losses:\n",
    "    loss_steps = [i for i, x in enumerate(df_episode['q_loss']) if x > 0]\n",
    "    ax4.plot(loss_steps, q_losses, 'orange', linewidth=2)\n",
    "    ax4.set_title('Q-Learning Loss')\n",
    "    ax4.set_xlabel('Training Step')\n",
    "    ax4.set_ylabel('MSE Loss')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'No hay datos de loss\\n(buffer muy peque√±o)', \n",
    "             ha='center', va='center', transform=ax4.transAxes, fontsize=12)\n",
    "    ax4.set_title('Q-Learning Loss')\n",
    "\n",
    "plt.suptitle('Resultados del Episodio de Prueba - DQN', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estad√≠sticas\n",
    "print(f\"üìä Estad√≠sticas del episodio:\")\n",
    "print(f\"  Error promedio: {abs(df_episode['error']).mean():.3f}m\")\n",
    "print(f\"  Error final: {abs(df_episode['error'].iloc[-1]):.3f}m\")\n",
    "print(f\"  Recompensa promedio: {df_episode['reward'].mean():.3f}\")\n",
    "print(f\"  Control promedio: {df_episode['control'].mean():.1f} L/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Paso 4: Entrenar M√∫ltiples Episodios Cortos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar varios episodios cortos para ver evoluci√≥n\n",
    "NUM_EPISODES = 10\n",
    "STEPS_PER_EPISODE = 30\n",
    "\n",
    "print(f\"üîÑ Entrenando {NUM_EPISODES} episodios cortos ({STEPS_PER_EPISODE} steps cada uno)...\")\n",
    "\n",
    "multi_episode_results = []\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"\\n--- Episodio {episode+1}/{NUM_EPISODES} ---\")\n",
    "    \n",
    "    episode_data, total_reward = train_short_episode(agent, tank, max_steps=STEPS_PER_EPISODE, verbose=False)\n",
    "    \n",
    "    # Calcular m√©tricas del episodio\n",
    "    df_ep = pd.DataFrame(episode_data)\n",
    "    avg_error = abs(df_ep['error']).mean()\n",
    "    final_error = abs(df_ep['error'].iloc[-1])\n",
    "    avg_reward = df_ep['reward'].mean()\n",
    "    \n",
    "    episode_summary = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'avg_error': avg_error,\n",
    "        'final_error': final_error,\n",
    "        'avg_reward': avg_reward,\n",
    "        'epsilon': agent.get_epsilon(),\n",
    "        'memory_size': len(agent.memory)\n",
    "    }\n",
    "    \n",
    "    multi_episode_results.append(episode_summary)\n",
    "    \n",
    "    print(f\"  Recompensa total: {total_reward:6.2f}\")\n",
    "    print(f\"  Error promedio:   {avg_error:6.3f}m\")\n",
    "    print(f\"  Epsilon:          {agent.get_epsilon():6.3f}\")\n",
    "    print(f\"  Memoria:          {len(agent.memory):6d} experiencias\")\n",
    "\n",
    "print(f\"\\n‚úÖ Entrenamiento m√∫ltiple completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar evoluci√≥n del entrenamiento\n",
    "df_multi = pd.DataFrame(multi_episode_results)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Recompensa total por episodio\n",
    "ax1.plot(df_multi['episode'], df_multi['total_reward'], 'bo-', linewidth=2, markersize=6)\n",
    "ax1.set_title('Evoluci√≥n de la Recompensa Total')\n",
    "ax1.set_xlabel('Episodio')\n",
    "ax1.set_ylabel('Recompensa Total')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Error promedio por episodio\n",
    "ax2.plot(df_multi['episode'], df_multi['avg_error'], 'ro-', linewidth=2, markersize=6, label='Error Promedio')\n",
    "ax2.plot(df_multi['episode'], df_multi['final_error'], 'r^--', linewidth=2, markersize=6, label='Error Final')\n",
    "ax2.set_title('Evoluci√≥n del Error')\n",
    "ax2.set_xlabel('Episodio')\n",
    "ax2.set_ylabel('Error (m)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Epsilon decay\n",
    "ax3.plot(df_multi['episode'], df_multi['epsilon'], 'mo-', linewidth=2, markersize=6)\n",
    "ax3.set_title('Decaimiento de Epsilon')\n",
    "ax3.set_xlabel('Episodio')\n",
    "ax3.set_ylabel('Epsilon (Exploraci√≥n)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Tama√±o de memoria\n",
    "ax4.plot(df_multi['episode'], df_multi['memory_size'], 'co-', linewidth=2, markersize=6)\n",
    "ax4.set_title('Crecimiento de la Memoria')\n",
    "ax4.set_xlabel('Episodio')\n",
    "ax4.set_ylabel('Experiencias Almacenadas')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Evoluci√≥n del Entrenamiento DQN - Episodios Cortos', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Resumen final\n",
    "print(f\"üìà Resumen del entrenamiento:\")\n",
    "print(f\"  Recompensa inicial: {df_multi['total_reward'].iloc[0]:.2f}\")\n",
    "print(f\"  Recompensa final:   {df_multi['total_reward'].iloc[-1]:.2f}\")\n",
    "print(f\"  Mejora:             {df_multi['total_reward'].iloc[-1] - df_multi['total_reward'].iloc[0]:+.2f}\")\n",
    "print(f\"  Error inicial:      {df_multi['avg_error'].iloc[0]:.3f}m\")\n",
    "print(f\"  Error final:        {df_multi['avg_error'].iloc[-1]:.3f}m\")\n",
    "print(f\"  Epsilon final:      {df_multi['epsilon'].iloc[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Paso 5: Probar Agente Entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar agente con exploraci√≥n m√≠nima\n",
    "print(\"üß™ Probando agente con exploraci√≥n m√≠nima...\")\n",
    "\n",
    "# Reducir epsilon temporalmente para ver comportamiento \"aprendido\"\n",
    "original_epsilon = agent.get_epsilon()\n",
    "agent.epsilon = 0.0  # Sin exploraci√≥n\n",
    "\n",
    "# Test con condiciones espec√≠ficas\n",
    "test_setpoint = 3.5\n",
    "test_initial = 1.5\n",
    "\n",
    "print(f\"Condiciones de prueba:\")\n",
    "print(f\"  Setpoint: {test_setpoint}m\")\n",
    "print(f\"  Nivel inicial: {test_initial}m\")\n",
    "print(f\"  Error inicial: {test_setpoint - test_initial:.1f}m\")\n",
    "\n",
    "# Reiniciar con interface Gym\n",
    "observation, info = tank.reset(options={'initial_level': test_initial, 'setpoint': test_setpoint})\n",
    "test_data = []\n",
    "\n",
    "for step in range(40):\n",
    "    # Acci√≥n del agente (sin exploraci√≥n)\n",
    "    pid_params = agent.select_action(observation, training=False)\n",
    "    \n",
    "    # Control\n",
    "    kp, ki, kd = pid_params\n",
    "    error = observation[2]\n",
    "    integral = observation[4]\n",
    "    derivative = observation[5]\n",
    "    \n",
    "    control_signal = kp * error + ki * integral + kd * derivative\n",
    "    control_signal = 4.0 + control_signal\n",
    "    control_signal = np.clip(control_signal, 0.0, tank.max_inflow)\n",
    "    \n",
    "    # Ejecutar paso con interface Gym\n",
    "    next_observation, reward, terminated, truncated, info = tank.step(control_signal)\n",
    "    \n",
    "    test_data.append({\n",
    "        'step': step,\n",
    "        'level': observation[0],\n",
    "        'setpoint': observation[1],\n",
    "        'error': observation[2],\n",
    "        'kp': kp,\n",
    "        'ki': ki,\n",
    "        'kd': kd,\n",
    "        'control': control_signal,\n",
    "        'reward': reward\n",
    "    })\n",
    "    \n",
    "    observation = next_observation\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(f\"  Step {step:2d}: Nivel={observation[0]:.2f}m, Error={error:.3f}m, \"\n",
    "              f\"PID=[{kp:.1f},{ki:.1f},{kd:.1f}]\")\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "# Restaurar epsilon original\n",
    "agent.epsilon = original_epsilon\n",
    "\n",
    "print(f\"\\n‚úÖ Prueba completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar prueba final\n",
    "df_test = pd.DataFrame(test_data)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Control de nivel\n",
    "ax1.plot(df_test['step'], df_test['level'], 'b-', linewidth=3, label='Nivel Real')\n",
    "ax1.plot(df_test['step'], df_test['setpoint'], 'r--', linewidth=2, label='Setpoint')\n",
    "ax1.fill_between(df_test['step'], df_test['level'], df_test['setpoint'], alpha=0.2)\n",
    "ax1.set_title('Control de Nivel - Agente Entrenado (Sin Exploraci√≥n)')\n",
    "ax1.set_ylabel('Nivel (m)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Error absoluto\n",
    "ax2.plot(df_test['step'], abs(df_test['error']), 'g-', linewidth=2)\n",
    "ax2.set_title('Error Absoluto')\n",
    "ax2.set_ylabel('|Error| (m)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Par√°metros PID seleccionados\n",
    "ax3.plot(df_test['step'], df_test['kp'], 'r-', label='Kp', linewidth=2)\n",
    "ax3.plot(df_test['step'], df_test['ki'], 'g-', label='Ki', linewidth=2)\n",
    "ax3.plot(df_test['step'], df_test['kd'], 'b-', label='Kd', linewidth=2)\n",
    "ax3.set_title('Par√°metros PID Aprendidos')\n",
    "ax3.set_xlabel('Step')\n",
    "ax3.set_ylabel('Valor')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Se√±al de control y recompensa\n",
    "ax4_twin = ax4.twinx()\n",
    "line1 = ax4.plot(df_test['step'], df_test['control'], 'purple', linewidth=2, label='Control')\n",
    "line2 = ax4_twin.plot(df_test['step'], df_test['reward'], 'orange', linewidth=2, label='Reward')\n",
    "ax4.set_title('Se√±al de Control y Recompensa')\n",
    "ax4.set_xlabel('Step')\n",
    "ax4.set_ylabel('Control (L/s)', color='purple')\n",
    "ax4_twin.set_ylabel('Reward', color='orange')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Evaluaci√≥n del Agente DQN Entrenado', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# M√©tricas finales\n",
    "settling_time = None\n",
    "tolerance = 0.1  # 10cm de tolerancia\n",
    "for i, error in enumerate(abs(df_test['error'])):\n",
    "    if error <= tolerance:\n",
    "        settling_time = i\n",
    "        break\n",
    "\n",
    "print(f\"üìä M√©tricas de rendimiento:\")\n",
    "print(f\"  Error final:        {abs(df_test['error'].iloc[-1]):.3f}m\")\n",
    "print(f\"  Error promedio:     {abs(df_test['error']).mean():.3f}m\")\n",
    "print(f\"  Error m√°ximo:       {abs(df_test['error']).max():.3f}m\")\n",
    "print(f\"  Tiempo estab.:      {settling_time if settling_time else 'No alcanzado'} steps\")\n",
    "print(f\"  Recompensa prom.:   {df_test['reward'].mean():.3f}\")\n",
    "print(f\"  Overshoot:          {(df_test['level'].max() - df_test['setpoint'].iloc[0]):.3f}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Paso 6: Ajustar Hiperpar√°metros (Interactivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para probar diferentes hiperpar√°metros r√°pidamente\n",
    "def quick_hyperparameter_test(lr=0.001, epsilon_decay=0.995, target_update_freq=50):\n",
    "    \"\"\"\n",
    "    Crear y probar agente con hiperpar√°metros espec√≠ficos\n",
    "    \"\"\"\n",
    "    print(f\"üîß Probando: lr={lr}, epsilon_decay={epsilon_decay}, target_update={target_update_freq}\")\n",
    "    \n",
    "    # Crear nuevo agente\n",
    "    test_agent = DQN_Agent(\n",
    "        state_dim=6,\n",
    "        lr=lr,\n",
    "        epsilon_decay=epsilon_decay,\n",
    "        target_update_freq=target_update_freq,\n",
    "        memory_size=500,  # Buffer peque√±o para pruebas r√°pidas\n",
    "        batch_size=16\n",
    "    )\n",
    "    \n",
    "    # Entrenar r√°pidamente\n",
    "    rewards = []\n",
    "    for ep in range(5):  # Solo 5 episodios\n",
    "        _, reward = train_short_episode(test_agent, tank, max_steps=20, verbose=False)\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    avg_reward = np.mean(rewards)\n",
    "    print(f\"  Recompensa promedio: {avg_reward:.2f}\")\n",
    "    \n",
    "    return avg_reward, rewards\n",
    "\n",
    "# Probar diferentes configuraciones\n",
    "print(\"üß™ Prueba r√°pida de hiperpar√°metros...\")\n",
    "\n",
    "configs = [\n",
    "    {'lr': 0.001, 'epsilon_decay': 0.995, 'target_update_freq': 50},\n",
    "    {'lr': 0.003, 'epsilon_decay': 0.990, 'target_update_freq': 30},\n",
    "    {'lr': 0.0005, 'epsilon_decay': 0.999, 'target_update_freq': 100}\n",
    "]\n",
    "\n",
    "results = []\n",
    "for i, config in enumerate(configs):\n",
    "    print(f\"\\n--- Configuraci√≥n {i+1} ---\")\n",
    "    avg_reward, rewards = quick_hyperparameter_test(**config)\n",
    "    results.append({\n",
    "        'config': config,\n",
    "        'avg_reward': avg_reward,\n",
    "        'rewards': rewards\n",
    "    })\n",
    "\n",
    "# Mostrar mejores resultados\n",
    "best_config = max(results, key=lambda x: x['avg_reward'])\n",
    "print(f\"\\nüèÜ Mejor configuraci√≥n:\")\n",
    "print(f\"  Par√°metros: {best_config['config']}\")\n",
    "print(f\"  Recompensa promedio: {best_config['avg_reward']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Conclusiones y Pr√≥ximos Pasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã RESUMEN DE LA EXPLORACI√ìN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n‚úÖ Lo que funciona:\")\n",
    "print(\"  - Simulador de tanque funciona con interface Gym\")\n",
    "print(\"  - Agente DQN se crea sin errores\")\n",
    "print(\"  - Experience replay funciona\")\n",
    "print(\"  - Target network se actualiza\")\n",
    "print(\"  - Discretizaci√≥n de acciones PID funciona\")\n",
    "print(\"  - Integraci√≥n completa DQN + Simulador\")\n",
    "\n",
    "print(\"\\nüîß Ajustes recomendados para entrenamiento completo:\")\n",
    "print(\"  - Aumentar episodes a 500+\")\n",
    "print(\"  - Aumentar steps por episodio a 200\")\n",
    "print(\"  - Aumentar buffer de memoria a 10000\")\n",
    "print(\"  - Usar mejores hiperpar√°metros encontrados\")\n",
    "\n",
    "print(\"\\nüìà M√©tricas observadas:\")\n",
    "if 'df_multi' in locals():\n",
    "    print(f\"  - Mejora en recompensa: {df_multi['total_reward'].iloc[-1] - df_multi['total_reward'].iloc[0]:+.1f}\")\n",
    "    print(f\"  - Reducci√≥n de error: {df_multi['avg_error'].iloc[0] - df_multi['avg_error'].iloc[-1]:+.3f}m\")\n",
    "    print(f\"  - Epsilon final: {df_multi['epsilon'].iloc[-1]:.3f}\")\n",
    "\n",
    "print(\"\\nüöÄ Pr√≥ximos pasos:\")\n",
    "print(\"  1. Ejecutar entrenamiento completo con train_dqn.py\")\n",
    "print(\"  2. Analizar m√©tricas de convergencia\")\n",
    "print(\"  3. Probar agente en condiciones m√°s complejas\")\n",
    "print(\"  4. Comparar con otros algoritmos (PPO, SAC)\")\n",
    "print(\"  5. Optimizar para deployment industrial\")\n",
    "\n",
    "print(\"\\nüíæ Para ejecutar entrenamiento completo:\")\n",
    "print(\"     cd simulations/tanque_nivel\")\n",
    "print(\"     python train_dqn.py\")\n",
    "\n",
    "print(\"\\nüéâ NOTEBOOK ACTUALIZADO CON INTERFACE GYM\")\n",
    "print(\"\" + \"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}